{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# “Common Interest”: no. of common articles between two users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_1 = pd.read_csv('data/Postings_01052019_15052019.csv', sep=';')\n",
    "postings_2 = pd.read_csv('data/Postings_16052019_31052019.csv', sep=';')\n",
    "postings = pd.concat([postings_1,postings_2])\n",
    "postings['PostingCreatedAt'] = postings.PostingCreatedAt.astype('datetime64')\n",
    "postings['ArticlePublishingDate'] = postings.ArticlePublishingDate.astype('datetime64')\n",
    "postings['UserCreatedAt'] = postings.UserCreatedAt.astype('datetime64')\n",
    "postings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(postings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings.UserCommunityName.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = postings[[\"ID_CommunityIdentity\",\"ID_Article\",\"PostingCreatedAt\",\"ArticleChannel\",\"ArticleRessortName\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day_diff = df[(df.PostingCreatedAt >= '2019-05-01') & \n",
    "                   (df.PostingCreatedAt < '2019-05-02')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day_diff.loc[df_day_diff['ID_CommunityIdentity'] == 117313]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day_diff.loc[df_day_diff['ID_CommunityIdentity'] == 571503]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day_diff.ArticleChannel.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day_diff.ID_CommunityIdentity.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day_diff.ID_Article.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.drop_duplicates(keep=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(edges.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    \n",
    "df_day = df[(df.PostingCreatedAt >= '2019-05-01') & \n",
    "                   (df.PostingCreatedAt < '2019-05-02')].merge(df, \n",
    "                    on = 'ID_Article', suffixes = ['_1', '_2'], how = 'inner')\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day = df_day[[\"ID_CommunityIdentity_1\",\"ID_Article\",\"ID_CommunityIdentity_2\", \"ArticleChannel_1\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day.drop_duplicates(keep=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countCommon(row):\n",
    "    channel1 = df_day_diff[\"ID_Article\"][df_day_diff[\"ID_CommunityIdentity\"]==row[\"source\"]]\n",
    "    channel2 = df_day_diff[\"ID_Article\"][df_day_diff[\"ID_CommunityIdentity\"]==row[\"target\"]]\n",
    "    return len(list(set(channel1).intersection(channel2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the combinations of countries from original Dataframe\n",
    "from itertools import combinations\n",
    "comb = combinations(df[\"ID_CommunityIdentity\"].unique(), 2)\n",
    "out = pd.DataFrame(list(comb), columns=[\"source\", \"target\"])\n",
    "\n",
    "# Find out the sports in common between coutries\n",
    "out[\"weight\"] = out.apply(countCommonSports, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = df_day[[\"ID_CommunityIdentity_1\",\"ID_CommunityIdentity_2\",\"ID_Article\",\"ArticleChannel_1\"]].groupby(\n",
    "            [\"ID_CommunityIdentity_1\",\"ID_CommunityIdentity_2\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges\n",
    "edges.reset_index(inplace = True)\n",
    "edges.rename(columns = {'ID_CommunityIdentity_1': 'source', 'ID_CommunityIdentity_2': 'target',\n",
    "                        'ID_Article': 'weight','ArticleChannel_1': 'ArticleChannel_1'}, inplace = True)\n",
    "edges\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = edges[edges.weight > 1]\n",
    "edges = edges[edges.source != edges.target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges=edges.loc[pd.DataFrame(np.sort(edges[['source','target']],1),index=edges.index).drop_duplicates(keep='first').index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(edges.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.to_csv(\"data/vote_to_edges_day_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_day_1 = pd.read_csv(\"data/vote_to_edges_day_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_day_1 = nx.from_pandas_edgelist(edges_day_1, edge_attr = 'weight', create_using=nx.DiGraph())\n",
    "G_day_1_undirected = nx.from_pandas_edgelist(edges_day_1, edge_attr = 'weight', create_using=nx.Graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_per_day = []\n",
    "\n",
    "for i in range(1,6):\n",
    "    df_day = df[(df.PostingCreatedAt >= '2019-05-{}'.format(str(i).zfill(2))) & \n",
    "                   (df.PostingCreatedAt < '2019-05-{}'.format(str(i+1).zfill(2)))].merge(df, \n",
    "                    on = 'ID_Article', suffixes = ['_1', '_2'], how = 'inner')\n",
    "    df_day = df_day[[\"ID_CommunityIdentity_1\",\"ID_Article\",\"ID_CommunityIdentity_2\"]]\n",
    "\n",
    "    df_day['day'] = i\n",
    "    df_day = df_day[['ID_CommunityIdentity_1', 'ID_Article', 'day', \n",
    "                     'ID_CommunityIdentity_2']]\n",
    "    df_day.drop_duplicates(keep=False,inplace=True)\n",
    "     \n",
    "    df_per_day.append(df_day)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_per_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputting the csv for source,target,weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_days = pd.DataFrame(columns = ['source', 'target', 'weight', 'day'])\n",
    "\n",
    "for i in range(5):\n",
    "    #df_per_day[i]=df_per_day[i][[\"ID_CommunityIdentity_1\",\"ID_CommunityIdentity_2\",\"ID_Article\",\"Day\"]]\n",
    "    df_per_day[i].drop_duplicates(keep=False,inplace=True)\n",
    "    edges_day = df_per_day[i][[\"ID_CommunityIdentity_1\",\"ID_CommunityIdentity_2\",\"ID_Article\",\"day\"]].groupby(\n",
    "            [\"ID_CommunityIdentity_1\",\"ID_CommunityIdentity_2\",\"day\"]).count()\n",
    "    print(\"1\")\n",
    "    edges_day.reset_index(inplace = True)\n",
    "    print(\"2\")\n",
    "    \n",
    "    edges_day.rename(columns = {'ID_CommunityIdentity_1': 'source', 'ID_CommunityIdentity_2': 'target',\n",
    "                        'ID_Article': 'weight'}, inplace = True)\n",
    "    #print(edges_day)\n",
    "    print(\"3\")\n",
    "    #edges_day.drop_duplicates(keep=False,inplace=True)\n",
    "    edges_day=edges_day.loc[pd.DataFrame(np.sort(edges_day[['source','target']],1),index=edges_day.index).drop_duplicates(keep='first').index]\n",
    "    edges_day = edges_day[edges_day.weight > 1]\n",
    "    edges_day = edges_day[edges_day.source != edges_day.target]\n",
    "    #edges_day = edges_day[edges_day.source != edges_day.target]\n",
    "    #edges_day['day'] = df_per_day[i].loc[0, 'Day']\n",
    "    print(\"4\")\n",
    "    print(edges_day)\n",
    "    edges_day.to_csv('edges_days.csv', mode='a',index=False)\n",
    "    #edges_days.iloc[len(edges_days.index):,:] = edges_day\n",
    "    #edges_days = pd.concat([edges_days, edges_day])\n",
    "    print(\"5\")\n",
    "    print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_days = pd.read_csv(\"edges_days.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_days.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTypeSeries = edges_days.dtypes\n",
    "print('Data type of each column of Dataframe :')\n",
    "print(dataTypeSeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_days['Day'] = edges_days['Day'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTypeSeries = edges_days.dtypes\n",
    "print('Data type of each column of Dataframe :')\n",
    "print(dataTypeSeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_days.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_days.day = pd.to_numeric(edges_days.day, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_days = nx.from_pandas_edgelist(edges_days, edge_attr = True, create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(G_days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_day = [(x,y,d) for x,y,d in G_days.edges.data() if d['day'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating features for separate days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def computation_input_df(graph,day):\n",
    "    # Creation of the input dataframe to return\n",
    "    input_df = pd.DataFrame(columns = ['day', 'source', 'target', 'jaccard_coef', 'adamic_adar_index',\n",
    "                                       'preferential_attachment_index', 'clustering_coefficient_score', 'weight'])\n",
    "    input_df[['day', 'source', 'target']] = input_df[['day', 'source', 'target']].astype('int')\n",
    "    input_df[['jaccard_coef', 'adamic_adar_index', 'preferential_attachment_index', \n",
    "              'clustering_coefficient_score', 'weight']] = input_df[['jaccard_coef', 'adamic_adar_index', \n",
    "              'preferential_attachment_index', 'clustering_coefficient_score', 'weight']].astype('float')\n",
    "    \n",
    "    # Creation of the subgraph for the day\n",
    "    edges_day = [(x,y,d) for x,y,d in graph.edges.data() if d['day'] == day]\n",
    "    print(edges_day)\n",
    "    sub_graph = nx.DiGraph()\n",
    "    sub_graph.add_edges_from(edges_day)\n",
    "    sub_graph_undirected = nx.Graph()\n",
    "    sub_graph_undirected.add_edges_from(edges_day)\n",
    "    nodes = sorted(sub_graph.nodes)\n",
    "    # Computation of Jaccard coefficient\n",
    "    jac = nx.jaccard_coefficient(sub_graph_undirected, [(i,j) for i,j in itertools.product(nodes, nodes) \n",
    "                                                        if i!=j])\n",
    "    jac_df = pd.DataFrame(list(jac), columns = ['source', 'target', 'jaccard_coef'])\n",
    "    jac_df['day'] = day\n",
    "    print('Jaccard: ok!')\n",
    "    # Computation of Adamic and Adar index\n",
    "    ad_ad = nx.adamic_adar_index(sub_graph_undirected, [(i,j) for i,j in itertools.product(nodes, nodes) \n",
    "                                                        if i!=j])\n",
    "    ad_ad_df = pd.DataFrame(list(ad_ad), columns = ['source', 'target', 'adamic_adar_index'])\n",
    "    print('Adamic and Adar: ok!')\n",
    "    input_df_day = jac_df.merge(ad_ad_df)\n",
    "    # Computation of Preferential Attachment\n",
    "    pref_att = nx.preferential_attachment(sub_graph_undirected, [(i,j) for i,j in itertools.product(nodes, \n",
    "                                                                  nodes) if i!=j])\n",
    "    pref_att_df = pd.DataFrame(list(pref_att), columns = ['source', 'target', 'preferential_attachment_index'])\n",
    "    print('Preferential Attachment: ok!')\n",
    "    \n",
    "    \n",
    "    input_df_day = input_df_day.merge(pref_att_df)\n",
    "    # Clustering coefficient score\n",
    "    \n",
    "    \n",
    "    #cluster_dict = nx.clustering(sub_graph)\n",
    "    #ccs = [(i, j, (cluster_dict[i]*cluster_dict[j])) for i,j in itertools.product(nodes, nodes) if i!=j]\n",
    "    #ccs_df = pd.DataFrame(list(ccs), columns = ['source', 'target', 'clustering_coefficient_score'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Clustering coefficient score (on directed graph)\n",
    "    cluster_dict = nx.clustering(sub_graph)\n",
    "    cluster_dict_df = pd.DataFrame.from_dict(cluster_dict, orient='index', columns=['clustering_coefficient_score']).reset_index().rename(columns={'index':'User'})\n",
    "    assert cluster_dict_df.shape[0] == len(nodes)\n",
    "    #display(cluster_dict_df.head())\n",
    "    # Add a column for clustering_coefficient_score_Source_User using the Clustering coefficient for the Source_User node\n",
    "    input_df_day = input_df_day.merge(cluster_dict_df.rename(columns={'User': 'source', \n",
    "                                                               'clustering_coefficient_score': 'clustering_coefficient_score_Source_User'}),\n",
    "                                on='source', how='left')\n",
    "    # Add a column for clustering_coefficient_score_Target_User using the Clustering coefficient for the Target_User node\n",
    "    input_df_day = input_df_day.merge(cluster_dict_df.rename(columns={'User': 'target', \n",
    "                                                               'clustering_coefficient_score': 'clustering_coefficient_score_Target_User'}),\n",
    "                                on='target', how='left')\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('Clustering coefficient: ok!')\n",
    "    \n",
    "    \n",
    "      # Pagerank (on directed graph)\n",
    "    pagerank_dict = nx.pagerank(sub_graph)\n",
    "    pagerank_dict_df = pd.DataFrame.from_dict(pagerank_dict, orient='index', columns=['pagerank']).reset_index().rename(columns={'index':'User'})\n",
    "    assert pagerank_dict_df.shape[0] == len(nodes)\n",
    "     # Add a column for pagerank_Source_User using the Pagerank for the Source_User node\n",
    "    input_df_day = input_df_day.merge(pagerank_dict_df.rename(columns={'User': 'source', \n",
    "                                                               'pagerank': 'pagerank_Source_User'}),\n",
    "                                on='source', how='left')\n",
    "    # Add a column for pagerank_Target_User using the Pagerank for the Target_User node\n",
    "    input_df_day = input_df_day.merge(pagerank_dict_df.rename(columns={'User': 'target', \n",
    "                                                               'pagerank': 'pagerank_Target_User'}),\n",
    "                                on='target', how='left')\n",
    "    \n",
    "    print('pagerank: ok!')\n",
    "    #input_df_day = input_df_day.merge(ccs_df)\n",
    "    # Add weights\n",
    "    weights = pd.DataFrame([(n1, n2, sub_graph.edges[n1,n2]['weight']) \n",
    "                                 if (n1,n2) in sub_graph.edges else (n1, n2, 0)\n",
    "                                 for n1 in nodes for n2 in nodes], \n",
    "                                 columns = ['source', 'target', 'weight'])\n",
    "    print('Weights: ok!')\n",
    "    input_df_day = input_df_day.merge(weights)\n",
    "    input_df_day.to_csv('data/input_df_day_{}.csv'.format(str(day)), index = False)\n",
    "    \n",
    "    return input_df_day\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computation_input_df(G_days,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating features for combined days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computation_input_df_combined(graph,start,end):\n",
    "    print('Days {}-{}'.format(str(start),str(end)))\n",
    "    # Creation of the input dataframe to return\n",
    "    input_df = pd.DataFrame(columns = ['day', 'source', 'target', 'jaccard_coef', 'adamic_adar_index',\n",
    "                                       'preferential_attachment_index', 'clustering_coefficient_score', 'weight'])\n",
    "    input_df[['day', 'source', 'target']] = input_df[['day', 'source', 'target']].astype('int')\n",
    "    input_df[['jaccard_coef', 'adamic_adar_index', 'preferential_attachment_index', \n",
    "              'clustering_coefficient_score', 'weight']] = input_df[['jaccard_coef', 'adamic_adar_index', \n",
    "              'preferential_attachment_index', 'clustering_coefficient_score', 'weight']].astype('float')\n",
    "    \n",
    "    # Creation of the subgraph for the day\n",
    "    edges_day = [(x,y,d) for x,y,d in G_days.edges.data() if (d['day']>=start) & (d['day']<=end)]\n",
    "    sub_graph = nx.DiGraph()\n",
    "    sub_graph.add_edges_from(edges_day)\n",
    "    sub_graph_undirected = nx.Graph()\n",
    "    sub_graph_undirected.add_edges_from(edges_day)\n",
    "    nodes = sorted(sub_graph.nodes)\n",
    "    # Computation of Jaccard coefficient\n",
    "    jac = nx.jaccard_coefficient(sub_graph_undirected, [(i,j) for i,j in itertools.product(nodes, nodes) \n",
    "                                                        if i!=j])\n",
    "    jac_df = pd.DataFrame(list(jac), columns = ['source', 'target', 'jaccard_coef'])\n",
    "    jac_df['day'] = '{}-{}'.format(str(start), str(end))\n",
    "    print('Jaccard: ok!')\n",
    "    # Computation of Adamic and Adar index\n",
    "    ad_ad = nx.adamic_adar_index(sub_graph_undirected, [(i,j) for i,j in itertools.product(nodes, nodes) \n",
    "                                                        if i!=j])\n",
    "    ad_ad_df = pd.DataFrame(list(ad_ad), columns = ['source', 'target', 'adamic_adar_index'])\n",
    "    print('Adamic and Adar: ok!')\n",
    "    input_df_day = jac_df.merge(ad_ad_df)\n",
    "    # Computation of Preferential Attachment\n",
    "    pref_att = nx.preferential_attachment(sub_graph_undirected, [(i,j) for i,j in itertools.product(nodes, \n",
    "                                                                  nodes) if i!=j])\n",
    "    pref_att_df = pd.DataFrame(list(pref_att), columns = ['source', 'target', 'preferential_attachment_index'])\n",
    "    print('Preferential Attachment: ok!')\n",
    "    input_df_day = input_df_day.merge(pref_att_df)\n",
    "    # Clustering coefficient score\n",
    "    #cluster_dict = nx.clustering(sub_graph)\n",
    "    #ccs = [(i, j, (cluster_dict[i]*cluster_dict[j])) for i,j in itertools.product(nodes, nodes) if i!=j]\n",
    "    #ccs_df = pd.DataFrame(list(ccs), columns = ['source', 'target', 'clustering_coefficient_score'])\n",
    "    #print('Clustering coefficient: ok!')\n",
    "    #input_df_day = input_df_day.merge(ccs_df)\n",
    "    \n",
    "    cluster_dict = nx.clustering(sub_graph)\n",
    "    cluster_dict_df = pd.DataFrame.from_dict(cluster_dict, orient='index', columns=['clustering_coefficient_score']).reset_index().rename(columns={'index':'User'})\n",
    "    assert cluster_dict_df.shape[0] == len(nodes)\n",
    "    #display(cluster_dict_df.head())\n",
    "    # Add a column for clustering_coefficient_score_Source_User using the Clustering coefficient for the Source_User node\n",
    "    input_df_day = input_df_day.merge(cluster_dict_df.rename(columns={'User': 'source', \n",
    "                                                               'clustering_coefficient_score': 'clustering_coefficient_score_Source_User'}),\n",
    "                                on='source', how='left')\n",
    "    # Add a column for clustering_coefficient_score_Target_User using the Clustering coefficient for the Target_User node\n",
    "    input_df_day = input_df_day.merge(cluster_dict_df.rename(columns={'User': 'target', \n",
    "                                                               'clustering_coefficient_score': 'clustering_coefficient_score_Target_User'}),\n",
    "                                on='target', how='left')\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('Clustering coefficient: ok!')\n",
    "    \n",
    "    \n",
    "      # Pagerank \n",
    "    pagerank_dict = nx.pagerank(sub_graph)\n",
    "    pagerank_dict_df = pd.DataFrame.from_dict(pagerank_dict, orient='index', columns=['pagerank']).reset_index().rename(columns={'index':'User'})\n",
    "    assert pagerank_dict_df.shape[0] == len(nodes)\n",
    "     # Add a column for pagerank_Source_User using the Pagerank for the Source_User node\n",
    "    input_df_day = input_df_day.merge(pagerank_dict_df.rename(columns={'User': 'source', \n",
    "                                                               'pagerank': 'pagerank_Source_User'}),\n",
    "                                on='source', how='left')\n",
    "    # Add a column for pagerank_Target_User using the Pagerank for the Target_User node\n",
    "    input_df_day = input_df_day.merge(pagerank_dict_df.rename(columns={'User': 'target', \n",
    "                                                               'pagerank': 'pagerank_Target_User'}),\n",
    "                                on='target', how='left')\n",
    "    \n",
    "    print('pagerank: ok!')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Add weights\n",
    "    weights = pd.DataFrame([(n1, n2, sub_graph.edges[n1,n2]['weight']) \n",
    "                                 if (n1,n2) in sub_graph.edges else (n1, n2, 0)\n",
    "                                 for n1 in nodes for n2 in nodes], \n",
    "                                 columns = ['source', 'target', 'weight'])\n",
    "    print('Weights: ok!')\n",
    "    input_df_day = input_df_day.merge(weights)\n",
    "    input_df_day.to_csv('input_df_day_{}_{}.csv'.format(str(start),str(end)), \n",
    "                        index = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computation_input_df_combined(G_days,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df_day = input_df_day.merge(weights)\n",
    "input_df_day.to_csv('input_df_day_{}_{}.csv'.format(str(1),str(1)), \n",
    "                        index = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNA_2021W",
   "language": "python",
   "name": "sna_2021w"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
