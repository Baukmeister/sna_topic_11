{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Network Analysis - Project sna_topic_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import compress\n",
    "import pickle\n",
    "from itertools import compress, product\n",
    "import datetime\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option(\"^display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:20,.4f}'.format)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "The data set is provided by Der Standard, one of the top Austrian newspapers.\n",
    "In the online Standard people can post comments below articles and up/down vote comments.\n",
    "\n",
    "1. There are two files containing all **postings** to articles in May 2019 (due to high amount of data, the postings were split into two files). The respective file also contains additionally meta-data of the postings and articles and some details about the users who composed the postings.\n",
    "\n",
    "2. There are two files containing all **votes** for the postings in point 1 (due to high amount of data, the votes were split into two files). The respective file also contains information whether the vote was negative or positive and some details about the user who did the voting.\n",
    "\n",
    "3. There is one file containing **following and ignoring relationships** among all the users who posted (see point 1) or voted (see point 2) to articles published in May 2019. A following relationship (i.e., the user with the `ID_CommunityIdentity` given in column 1 follows the user with the `ID_CommunityIdentityConnectedTo` given in column 2) is indicated by a “1” in column the `“ID_CommunityConnectionType”`, a ignoring relationship by a “2” in that column (i.e., the user with the `ID_CommunityIdentity` given in column 1 ignores the user with the `ID_CommunityIdentityConnectedTo` given in column 2).\n",
    "\n",
    "There are different entities in the data set: \n",
    "* **Users** - identified by *ID_CommunityIdentity* (or *UserCommunityName*)\n",
    "* **Postings** - identified by *ID_Posting*\n",
    "* **Articles** - identified by *ID_Article*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to datasets:\n",
    "file_postings_1 = 'data/Postings_01052019_15052019.csv'\n",
    "file_postings_2 = 'data/Postings_16052019_31052019.csv'\n",
    "file_votes_1 = 'data/Votes_01052019_15052019.csv'\n",
    "file_votes_2 = 'data/Votes_16052019_31052019.csv'\n",
    "file_following_ignoring = 'data/Following_Ignoring_Relationships_01052019_31052019.csv'\n",
    "\n",
    "output_dir = 'output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_postings_1 = pd.read_csv(file_postings_1, sep=';')\n",
    "display(df_postings_1.head(2))\n",
    "df_postings_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_postings_2 = pd.read_csv(file_postings_2, sep=';')\n",
    "display(df_postings_2.head(2))\n",
    "df_postings_2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Concat Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat postings1 and postings2 in order! \n",
    "df_postings = pd.concat([df_postings_1, df_postings_2], ignore_index=True)\n",
    "\n",
    "df_postings[['PostingCreatedAt', 'ArticlePublishingDate', 'UserCreatedAt']] = df_postings[['PostingCreatedAt', 'ArticlePublishingDate', 'UserCreatedAt']].astype('datetime64')\n",
    "\n",
    "df_postings.info()\n",
    "df_postings.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO is it necessary to subset the data because it is too large?\n",
    "\n",
    "#df_postings['PostingCreatedAt'].dt.date.head()\n",
    "# df_postings = df_postings[pd.to_datetime(df_postings['PostingCreatedAt'].dt.date) == '2019-05-01']\n",
    "df_postings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation 1: User_A commented/posted to post of User_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Edge relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of iterating over the dataframe rows to create a edge list, I use a join, which is way faster on the whole dataset. <br>\n",
    "But, by default `merge` changes the sort order, so you have to sort by the original dataframe index!<br>\n",
    "The following code creates a dataframe for posting source user and target posting user. It has the same shape as the original dataframe `df_postings`.\n",
    "So, target posting user may be `NaN`, due to left join and preserving all observations in the posting dataframe.\n",
    "One could filter the Target_User column by not `NaN` to get only relations between users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_user_commented_relation_df = df_postings.reset_index()[['index', 'ID_Posting', 'ID_CommunityIdentity', 'PostingCreatedAt', 'ID_Posting_Parent']].merge(\n",
    "    df_postings[['ID_Posting', 'ID_CommunityIdentity']], \n",
    "    left_on='ID_Posting_Parent', right_on='ID_Posting',\n",
    "    suffixes=('', '_parent'), how='left', sort=False\n",
    ")[['index', 'ID_Posting', 'PostingCreatedAt', 'ID_CommunityIdentity', 'ID_CommunityIdentity_parent']].sort_values(by='index')\n",
    "\n",
    "postings_user_commented_relation_df.set_index('index', inplace=True) # index may be useful for selection?\n",
    "# postings_user_commented_relation_df.set_index(['ID_Posting', 'PostingCreatedAt'], inplace=True) # index may be useful for selection?\n",
    "postings_user_commented_relation_df.rename(columns={'ID_CommunityIdentity': 'Source_User', 'ID_CommunityIdentity_parent': 'Target_User'}, inplace=True)\n",
    "\n",
    "# postings_user_commented_relation_df.drop('index', axis='columns', inplace=True) # index is ascending as original df_postings, index column not needed anymore\n",
    "\n",
    "# SAME number of rows and SAME order as original df_postings:\n",
    "print(df_postings.shape)\n",
    "print(postings_user_commented_relation_df.shape)\n",
    "assert df_postings.shape[0] == postings_user_commented_relation_df.shape[0]\n",
    "\n",
    "# add column PostingCreatedAtDay to have easy access to Day of month\n",
    "postings_user_commented_relation_df['PostingCreatedAtDay'] = postings_user_commented_relation_df['PostingCreatedAt'].dt.day\n",
    "\n",
    "display(postings_user_commented_relation_df.head(5))\n",
    "display(postings_user_commented_relation_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_created_at(series, day_start, day_end):\n",
    "    \"\"\"\n",
    "    create a boolean mask of series where day is in day_start incl. and day_end_incl.\n",
    "    \"\"\"\n",
    "    return series.isin(range(day_start, day_end+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_posting_created_at = get_mask_created_at(postings_user_commented_relation_df['PostingCreatedAtDay'], \n",
    "                                                    day_start=2, day_end=4)\n",
    "\n",
    "display(postings_user_commented_relation_df.loc[mask_posting_created_at].sample(10))\n",
    "list(postings_user_commented_relation_df.loc[mask_posting_created_at]['PostingCreatedAtDay'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful when using the df_posting dataframe without `NAN` values below. Some operations require the original `df_postings` order (NLP text extraction)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_user_commented_relation_nonan_df = postings_user_commented_relation_df.dropna()\n",
    "# check number of rows after dropping NAN\n",
    "print(postings_user_commented_relation_df.shape)\n",
    "print(postings_user_commented_relation_nonan_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute graph topology features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function can compute graph features based on posting relation data frame and start and end day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computation_input_df_combined_postings(postings_relation_df, day_start, day_end):\n",
    "    \"\"\"\n",
    "    day_start incl. and day_end incl.\n",
    "    \"\"\"\n",
    "    print('computation_input_df_combined_postings: Days {}-{}'.format(str(day_start),str(day_end)))\n",
    "    assert (day_start <= day_end) & (day_start > 0) & (day_end <= 31)\n",
    "\n",
    "    postings_relation_df = postings_relation_df.copy()\n",
    "\n",
    "    # get postings relations where day is between day_start and day_end\n",
    "    mask_posting_created_at = get_mask_created_at(postings_relation_df['PostingCreatedAtDay'], \n",
    "                                                  day_start=day_start, day_end=day_end)\n",
    "    postings_relation_df_day_subset = postings_relation_df.loc[mask_posting_created_at]\n",
    "    # assert all postings are created between day_start and day_end:\n",
    "    assert postings_relation_df_day_subset['PostingCreatedAtDay'].isin(range(day_start, day_end+1)).all()\n",
    "\n",
    "    # display(postings_relation_df_day_subset.sample(4))\n",
    "\n",
    "    # drop NaN in Target_User, because these rows have no relation:\n",
    "    postings_relation_df_day_subset_no_nan = postings_relation_df_day_subset.dropna().copy()\n",
    "    postings_relation_df_day_subset_no_nan['Target_User'] = postings_relation_df_day_subset_no_nan['Target_User'].astype('int64')\n",
    "\n",
    "    #display(postings_relation_df_day_subset_no_nan.sample(4))\n",
    "    #display(postings_relation_df_day_subset_no_nan.info())\n",
    "\n",
    "    # obtain interaction count as edge weight\n",
    "    postings_relation_df_day_subset_no_nan_count = postings_relation_df_day_subset_no_nan[['ID_Posting', 'Source_User', 'Target_User']].groupby(\n",
    "                                                        ['Source_User', 'Target_User'])['ID_Posting'].count().reset_index(name=\"weight\")\n",
    "\n",
    "    #display(postings_relation_df_day_subset_no_nan_count.sample(4))\n",
    "    #display(postings_relation_df_day_subset_no_nan_count.info())\n",
    "\n",
    "    #Creation of directed graph for day range\n",
    "    G_postings_commented_days_sub_directed = nx.from_pandas_edgelist(postings_relation_df_day_subset_no_nan_count,\n",
    "                                                                     source='Source_User',\n",
    "                                                                     target='Target_User',\n",
    "                                                                     edge_attr='weight',\n",
    "                                                                     create_using=nx.DiGraph())\n",
    "    print('Created a', nx.info(G_postings_commented_days_sub_directed))\n",
    "    # assert that the graph has weight as edge attribute with a weight > 0 (should be weight column in df)\n",
    "    assert G_postings_commented_days_sub_directed.get_edge_data(*tuple(postings_relation_df_day_subset_no_nan_count.loc[0, ['Source_User', 'Target_User']]))['weight'] > 0\n",
    "\n",
    "    G_postings_commented_days_sub_undirected = nx.from_pandas_edgelist(postings_relation_df_day_subset_no_nan_count,\n",
    "                                                                       source='Source_User',\n",
    "                                                                       target='Target_User',\n",
    "                                                                       create_using=nx.Graph())\n",
    "    print('Created a', nx.info(G_postings_commented_days_sub_undirected))\n",
    "\n",
    "    # node list, ebunch is pairs of node and should be of length (len(nodes)*(len(nodes)-1)):\n",
    "    nodes = list(G_postings_commented_days_sub_directed.nodes)\n",
    "    ebunch = [(i,j) for i,j in product(nodes, nodes) if i!=j] # all node pairs\n",
    "    assert len(ebunch) == (len(nodes)*(len(nodes)-1))\n",
    "\n",
    "    print(f'{datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} Starting computation')\n",
    "\n",
    "    # Computation of Jaccard coefficient\n",
    "    jac = nx.jaccard_coefficient(G_postings_commented_days_sub_undirected, ebunch)\n",
    "    jac_df = pd.DataFrame(list(jac), columns = ['Source_User', 'Target_User', 'jaccard_coef'])\n",
    "    assert jac_df.shape[0] == (len(nodes)*(len(nodes)-1))\n",
    "    print(f'{datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} Jaccard: ok!')\n",
    "\n",
    "    # Computation of Adamic and Adar index\n",
    "    ad_ad = nx.adamic_adar_index(G_postings_commented_days_sub_undirected, ebunch)\n",
    "    ad_ad_df = pd.DataFrame(list(ad_ad), columns = ['Source_User', 'Target_User', 'adamic_adar_index'])\n",
    "    assert ad_ad_df.shape[0] == (len(nodes)*(len(nodes)-1))\n",
    "    print(f'{datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} Adamic and Adar: ok!')\n",
    "    result_df = jac_df.merge(ad_ad_df)\n",
    "\n",
    "    # Computation of Preferential Attachment\n",
    "    pref_att = nx.preferential_attachment(G_postings_commented_days_sub_undirected, ebunch)\n",
    "    pref_att_df = pd.DataFrame(list(pref_att), columns = ['Source_User', 'Target_User', 'preferential_attachment_index'])\n",
    "    assert pref_att_df.shape[0] == (len(nodes)*(len(nodes)-1))\n",
    "    print(f'{datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} Preferential Attachment: ok!')\n",
    "    result_df = result_df.merge(pref_att_df)\n",
    "\n",
    "    # Clustering coefficient score (on directed graph)\n",
    "    cluster_dict = nx.clustering(G_postings_commented_days_sub_directed)\n",
    "    cluster_dict_df = pd.DataFrame.from_dict(cluster_dict, orient='index', columns=['clustering_coefficient_score']).reset_index().rename(columns={'index':'User'})\n",
    "    assert cluster_dict_df.shape[0] == len(nodes)\n",
    "    #display(cluster_dict_df.head())\n",
    "    # Add a column for clustering_coefficient_score_Source_User using the Clustering coefficient for the Source_User node\n",
    "    result_df = result_df.merge(cluster_dict_df.rename(columns={'User': 'Source_User', \n",
    "                                                               'clustering_coefficient_score': 'clustering_coefficient_score_Source_User'}),\n",
    "                                on='Source_User', how='left')\n",
    "    # Add a column for clustering_coefficient_score_Target_User using the Clustering coefficient for the Target_User node\n",
    "    result_df = result_df.merge(cluster_dict_df.rename(columns={'User': 'Target_User', \n",
    "                                                               'clustering_coefficient_score': 'clustering_coefficient_score_Target_User'}),\n",
    "                                on='Target_User', how='left')\n",
    "    print(f'{datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} Clustering coefficient: ok!')\n",
    "    \n",
    "    # Pagerank (on directed graph)\n",
    "    pagerank_dict = nx.pagerank(G_postings_commented_days_sub_directed)\n",
    "    pagerank_dict_df = pd.DataFrame.from_dict(pagerank_dict, orient='index', columns=['pagerank']).reset_index().rename(columns={'index':'User'})\n",
    "    assert pagerank_dict_df.shape[0] == len(nodes)\n",
    "     # Add a column for pagerank_Source_User using the Pagerank for the Source_User node\n",
    "    result_df = result_df.merge(pagerank_dict_df.rename(columns={'User': 'Source_User', \n",
    "                                                               'pagerank': 'pagerank_Source_User'}),\n",
    "                                on='Source_User', how='left')\n",
    "    # Add a column for pagerank_Target_User using the Pagerank for the Target_User node\n",
    "    result_df = result_df.merge(pagerank_dict_df.rename(columns={'User': 'Target_User', \n",
    "                                                               'pagerank': 'pagerank_Target_User'}),\n",
    "                                on='Target_User', how='left')\n",
    "    print(f'{datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} Pagerank: ok!')\n",
    "    print('Resulting result_df has shape', result_df.shape)\n",
    "    assert result_df.shape[0] == (len(nodes)*(len(nodes)-1))\n",
    "    display(result_df.head(5))\n",
    "    #display(result_df[result_df['clustering_coefficient_score_Target_User'] > 0])\n",
    "\n",
    "    # Write resulting graph topology features for Source_User and Target_User combinations to a file\n",
    "    pickle_out_file_path_features = 'output/G_features_postings_days_{}-{}.pkl'.format(str(day_start), str(day_end))\n",
    "    with open(pickle_out_file_path_features, 'wb') as f:\n",
    "        pickle.dump(result_df, f)\n",
    "    print(f'{datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} Successfully written result_df to file \"{pickle_out_file_path_features}\".')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute graph features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate for days and save to file. Not needed anymore - file is computed and in Google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_graph_features = computation_input_df_combined_postings(postings_user_commented_relation_df, day_start=1, day_end=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with some NLP extractions of text and one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download spacy german language model - used for tokenization, lemmatization of article text (vector data)\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot encoding (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot encoding of `'ArticleChannel'`, `'ArticleRessortName'` for ML pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# encode article channel and article ressort name with one hot encoding\n",
    "\n",
    "# TODO use it like that or use the TEST set for one hot encoding. \n",
    "# Assuming all article channels and article resort names are known a priori this is ok imho for derStandard.\n",
    "# Just use a subset of df_posting e.g. certain days, if the number of features is too high in ML pipeline...\n",
    "\n",
    "\n",
    "# TODO be aware of the order when adding it\n",
    "\n",
    "# ArticleChannel\tArticleRessortName\n",
    "one_hot_encoder_article_channel = OneHotEncoder(handle_unknown='ignore').fit(df_postings[['ArticleChannel', 'ArticleRessortName']])\n",
    "\n",
    "print(one_hot_encoder_article_channel.categories_)\n",
    "\n",
    "transformed_channel_resort = one_hot_encoder_article_channel.transform(df_postings[['ArticleChannel', 'ArticleRessortName']])\n",
    "print(transformed_channel_resort.shape)\n",
    "display(transformed_channel_resort.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF feature extraction (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF feature extraction of combined `PostingHeadline`, `PostingComment` and `ArticleTitle` next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_spacy = 'output/combinedPostingText.spacy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step for creating `'output/combinedPostingText.spacy'` is usually not necessary. It takes around 64 minutes and you can just use my file: <br>\n",
    "Run the next chunk, if you want to wait long for computing the `\"combinedPostingText.spacy\"` file.<br>\n",
    "`doc_bin` holds document information in order of `df_postings`. Existing `combinedPostingText.spacy` stores nlp information for all `df_postings` rows, not just a subset of days. Take care of the order of `df_postings`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "df_postings['CombinedArticlePostingText'] = df_postings['PostingHeadline'].fillna('') + ' ' + df_postings['PostingComment'].fillna('') + ' ' + df_postings['ArticleTitle'].fillna('')\n",
    "\n",
    "doc_bin = DocBin(attrs=[\"LEMMA\", \"ENT_IOB\", \"ENT_TYPE\"], store_user_data=True)\n",
    "\n",
    "for doc in nlp.pipe(df_postings['CombinedArticlePostingText'].str.lower()):\n",
    "    #print(repr(doc))\n",
    "    doc_bin.add(doc)\n",
    "\n",
    "#bytes_data = doc_bin.to_bytes()\n",
    "#doc_bin = DocBin().from_bytes(bytes_data)\n",
    "\n",
    "doc_bin.to_disk(file_name_spacy)\n",
    "\n",
    "# save tokenized spacy Doc objects to file\n",
    "# the file contains the combined text of posting headline, posting comment and article title in a tokenized form\n",
    "# and also meta data about a token, like its lemma form, whether a token is a punctuation, or bracket etc.\n",
    "# processing took around 64 minutes on the whole df_postings for me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next takes around 5 minutes for me. You could also just use the pickle file `tokenized_lemmatized_texts.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "doc_bin = DocBin().from_disk(file_name_spacy)\n",
    "\n",
    "docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "#print(docs)\n",
    "print(len(docs))\n",
    "tokenized_lemmatized_texts = [[token.lemma_ for token in doc \n",
    "                               if not token.is_stop and not token.is_punct and not token.is_space \n",
    "                               and not token.like_url and not token.like_email and not token.is_quote \n",
    "                               and not token.is_currency and not token.is_bracket and not token.is_quote\n",
    "                               and not token.is_left_punct and not token.is_right_punct and not token.is_bracket]\n",
    "                               for doc in docs]\n",
    "print(len(tokenized_lemmatized_texts))\n",
    "print(tokenized_lemmatized_texts[0])\n",
    "print(tokenized_lemmatized_texts[100])\n",
    "\n",
    "# tokenized_lemmatized_texts contains nlp information for all documents in df_postings in order!\n",
    "\n",
    "# save as pickle file:\n",
    "with open('output/tokenized_lemmatized_texts.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenized_lemmatized_texts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Continue here**: Load the pickled file `tokenized_lemmatized_texts.pkl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/tokenized_lemmatized_texts.pkl', 'rb') as f:\n",
    "    tokenized_lemmatized_texts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_lemmatized_texts contains nlp information for all documents in df_postings in order!\n",
    "# is a list of list of tokens, a list of tokens for the combined texts for a posting-article-combination for a certain posting entry in posting_df.\n",
    "print(tokenized_lemmatized_texts[0:2])\n",
    "print(len(tokenized_lemmatized_texts))\n",
    "print(len(df_postings))\n",
    "\n",
    "assert len(tokenized_lemmatized_texts) == len(df_postings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show an example of utilizing the NLP tf-idf features by adding them to the matrix of graph features.<br>\n",
    "The example is illustrated with **days 1–3 combined**. We may use these graph feature for Training and Testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set - graph based features used are from days 1--3 combined.\n",
    "# training set - vetor-based NLP features used are from days 1--3:\n",
    "\n",
    "# filter tokenized_lemmatized_texts by days of PostingCreatedAt:\n",
    "mask_posting_created_at_days_1to3 = get_mask_created_at(postings_user_commented_relation_df['PostingCreatedAtDay'],\n",
    "                                                        day_start=1, day_end=3)\n",
    "# filter target user is not nan\n",
    "mask_target_user_non_nan = ~postings_user_commented_relation_df['Target_User'].isna()\n",
    "\n",
    "mask_posting_days_1to3_target_user_non_nan = (mask_posting_created_at_days_1to3 & mask_target_user_non_nan)\n",
    "\n",
    "tokenized_lemmatized_texts_at_days_1to3 = list(compress(tokenized_lemmatized_texts, list(mask_posting_days_1to3_target_user_non_nan)))\n",
    "assert (len(tokenized_lemmatized_texts_at_days_1to3)) == mask_posting_days_1to3_target_user_non_nan.sum()\n",
    "display(tokenized_lemmatized_texts_at_days_1to3[0:1])\n",
    "print('There are', len(tokenized_lemmatized_texts_at_days_1to3),'tokenized-lemmatized texts for these days where Target_User is not NaN')\n",
    "\n",
    "assert postings_user_commented_relation_df.loc[mask_posting_days_1to3_target_user_non_nan].isna().sum(axis=0).sum() == 0\n",
    "assert postings_user_commented_relation_df.loc[mask_posting_days_1to3_target_user_non_nan]['PostingCreatedAtDay'].isin(range(1, 4+1)).all()\n",
    "assert ~postings_user_commented_relation_df.loc[mask_posting_days_1to3_target_user_non_nan]['PostingCreatedAtDay'].isin(range(5, 31+1)).all()\n",
    "assert len(tokenized_lemmatized_texts_at_days_1to3) == postings_user_commented_relation_df.loc[mask_posting_days_1to3_target_user_non_nan].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tf_idf_features = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO train test split!\n",
    "# only fit on train data and not on test data!\n",
    "\n",
    "# fit tokenized-lemmatized text for days subset on train data:\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), lowercase=False, tokenizer=lambda x: x, max_features=max_tf_idf_features)\n",
    "vectorizer = tf_idf_vectorizer.fit(tokenized_lemmatized_texts_at_days_1to3) # training data\n",
    "print(list(tf_idf_vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain tf-idf vectorized text features for training set:\n",
    "\n",
    "text_tf_idf_features_train = tf_idf_vectorizer.transform(tokenized_lemmatized_texts_at_days_1to3).toarray() # training or test data (here training)\n",
    "print('TF-IDF vectorized text features have shape:', text_tf_idf_features_train.shape)\n",
    "\n",
    "assert len(tokenized_lemmatized_texts_at_days_1to3) == text_tf_idf_features_train.shape[0]\n",
    "assert max_tf_idf_features == text_tf_idf_features_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(text_tf_idf_features_train).add_suffix('_TFIDF').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append tf-idf features to the right of postings_user_commented_relation_df content\n",
    "# BE CAREFUL OF THE ORDER!\n",
    "\n",
    "display(postings_user_commented_relation_df.loc[mask_posting_days_1to3_target_user_non_nan].reset_index(drop=True).head())\n",
    "print(postings_user_commented_relation_df.loc[mask_posting_days_1to3_target_user_non_nan].reset_index(drop=True).shape)\n",
    "\n",
    "display(pd.DataFrame(text_tf_idf_features_train).add_suffix('_TFIDF').head())\n",
    "print(pd.DataFrame(text_tf_idf_features_train).shape)\n",
    "\n",
    "# append features\n",
    "postings_user_commented_relation_df_with_tfidf_features = pd.concat([\n",
    "    postings_user_commented_relation_df.loc[mask_posting_days_1to3_target_user_non_nan].reset_index(drop=True), \n",
    "    pd.DataFrame(text_tf_idf_features_train).add_suffix('_TFIDF')], axis=1, sort=False)\n",
    "\n",
    "postings_user_commented_relation_df_with_tfidf_features['Target_User'] = postings_user_commented_relation_df_with_tfidf_features['Target_User'].astype('int64')\n",
    "\n",
    "display(postings_user_commented_relation_df_with_tfidf_features)\n",
    "print(postings_user_commented_relation_df_with_tfidf_features.shape)\n",
    "\n",
    "assert postings_user_commented_relation_df_with_tfidf_features.shape[0] == text_tf_idf_features_train.shape[0]\n",
    "assert postings_user_commented_relation_df_with_tfidf_features.shape[1] == (text_tf_idf_features_train.shape[1] + postings_user_commented_relation_df.loc[mask_posting_days_1to3_target_user_non_nan].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to copy the pickle file from our shared Google Drive folder\n",
    "with open('output/G_features_postings_days_1-3.pkl', 'rb') as f:\n",
    "    G_features_postings_days_1to3 = pickle.load(f)\n",
    "# dataframe of graph features for Source_User and Target_User\n",
    "G_features_postings_days_1to3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_tfidf_and_graph_features_only_positive = pd.merge(postings_user_commented_relation_df_with_tfidf_features, G_features_postings_days_1to3,\n",
    "                                                           on=['Source_User', 'Target_User'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(postings_tfidf_and_graph_features_only_positive.shape)\n",
    "display(postings_tfidf_and_graph_features_only_positive.head())\n",
    "print('Number of NaN in dataframe:')\n",
    "display(postings_tfidf_and_graph_features_only_positive.isna().sum()[postings_tfidf_and_graph_features_only_positive.isna().sum() > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can just drop rows with NaN. 934 rows with NaN values is not much compared to 46529 rows in total. (2%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(postings_tfidf_and_graph_features_only_positive.head())\n",
    "print(postings_tfidf_and_graph_features_only_positive.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_positive = pd.DataFrame(np.ones((postings_tfidf_and_graph_features_only_positive.shape[0], 1)), columns=['label']) # only 1s for positive samples\n",
    "\n",
    "postings_tfidf_and_graph_features_only_positive_with_label = pd.concat([postings_tfidf_and_graph_features_only_positive, label_positive], axis=1)\n",
    "assert postings_tfidf_and_graph_features_only_positive_with_label.shape == (postings_tfidf_and_graph_features_only_positive.shape[0], postings_tfidf_and_graph_features_only_positive.shape[1]+1)\n",
    "postings_tfidf_and_graph_features_only_positive_with_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN - some metrics are strangely NaN\n",
    "postings_tfidf_and_graph_features_only_positive_with_label.dropna(inplace=True)\n",
    "\n",
    "# drop posting id column\n",
    "postings_tfidf_and_graph_features_only_positive_with_label.drop('ID_Posting', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_tfidf_and_graph_features_only_positive_with_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result to file:\n",
    "with open('output/postings_tfidf_and_graph_features_only_positive_days_1to3.pkl', 'wb') as f:\n",
    "    pickle.dump(postings_tfidf_and_graph_features_only_positive_with_label, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load result from file\n",
    "with open('output/postings_tfidf_and_graph_features_only_positive_days_1to3.pkl', 'rb') as f:\n",
    "    postings_tfidf_and_graph_features_only_positive_with_label = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### negative sampling train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_post_creation_date = ['PostingCreatedAt', 'PostingCreatedAtDay']\n",
    "cols_source_target_user = ['Source_User', 'Target_User']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_tfidf_and_graph_features_only_positive_with_label['Target_User'].drop_duplicates().sample(frac=2, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples for Source_User and Target_User\n",
    "\n",
    "n = postings_tfidf_and_graph_features_only_positive_with_label.shape[0] * 3\n",
    "sampled_source_target_user = pd.concat([postings_tfidf_and_graph_features_only_positive_with_label['Source_User'].drop_duplicates().sample(n=n, replace=True, random_state=42).reset_index(drop=True),\n",
    "                                       postings_tfidf_and_graph_features_only_positive_with_label['Target_User'].drop_duplicates().sample(n=n, replace=True, random_state=42).reset_index(drop=True)], \n",
    "                                       axis=1)\n",
    "print(sampled_source_target_user.shape)\n",
    "\n",
    "\n",
    "mask_sample_pair_not_in_positives = ~sampled_source_target_user.isin(postings_tfidf_and_graph_features_only_positive_with_label[cols_source_target_user]).all(axis=1)\n",
    "#display(mask_sample_pair_not_in_positives)\n",
    "\n",
    "# drop rows where source=target user\n",
    "mask_source_user_equals_not_target_user = ~((sampled_source_target_user['Source_User'] == sampled_source_target_user['Target_User']))\n",
    "sampled_source_target_user = sampled_source_target_user.loc[mask_source_user_equals_not_target_user]\n",
    "\n",
    "sampled_source_target_user = sampled_source_target_user.loc[mask_sample_pair_not_in_positives].reset_index(drop=True)\n",
    "display(sampled_source_target_user)\n",
    "print(sampled_source_target_user.shape)\n",
    "assert sampled_source_target_user.shape[0] > 0 # samples of Source_User and Target_User not in postive dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples for day\n",
    "n = sampled_source_target_user.shape[0]\n",
    "sampled_posting_creation_date = postings_tfidf_and_graph_features_only_positive_with_label[cols_post_creation_date].drop_duplicates().sample(n=n, replace=True, random_state=42).reset_index(drop=True)\n",
    "display(sampled_posting_creation_date)\n",
    "assert (sampled_posting_creation_date.shape[0] == n) & (sampled_posting_creation_date.shape[1] == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat day and user data:\n",
    "\n",
    "print(sampled_posting_creation_date.shape)\n",
    "print(sampled_source_target_user.shape)\n",
    "\n",
    "display(sampled_posting_creation_date.head())\n",
    "display(sampled_source_target_user.head())\n",
    "\n",
    "label_negative = pd.DataFrame(np.zeros((sampled_posting_creation_date.shape[0], 1)), columns=['label']) # only 0s for negative samples\n",
    "\n",
    "postings_only_negative_with_label = pd.concat([sampled_posting_creation_date, sampled_source_target_user, label_negative], axis=1)\n",
    "\n",
    "print(postings_only_negative_with_label.shape)\n",
    "assert (postings_only_negative_with_label.shape[0] == sampled_posting_creation_date.shape[0])\n",
    "assert (postings_only_negative_with_label.shape[1] == 5)\n",
    "assert (postings_only_negative_with_label['PostingCreatedAt'].dt.day == postings_only_negative_with_label['PostingCreatedAtDay']).all()\n",
    "display(postings_only_negative_with_label.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(pd.DataFrame(text_tf_idf_features_train).add_suffix('_TFIDF').head())\n",
    "#print(pd.DataFrame(text_tf_idf_features_train).shape)\n",
    "\n",
    "n = pd.DataFrame(text_tf_idf_features_train).shape[0]\n",
    "\n",
    "np.random.seed(42)\n",
    "sample_tf_idf_features_negative_indices = np.random.choice(np.arange(0, n), size=postings_only_negative_with_label.shape[0], replace=True)\n",
    "print(sample_tf_idf_features_negative_indices)\n",
    "assert len(sample_tf_idf_features_negative_indices) == postings_only_negative_with_label.shape[0]\n",
    "\n",
    "sample_tf_idf_features_negative = pd.DataFrame(text_tf_idf_features_train).add_suffix('_TFIDF').loc[sample_tf_idf_features_negative_indices].reset_index(drop=True)\n",
    "display(sample_tf_idf_features_negative.head())\n",
    "print(sample_tf_idf_features_negative.shape)\n",
    "assert sample_tf_idf_features_negative.shape[0] == postings_only_negative_with_label.shape[0]\n",
    "assert sample_tf_idf_features_negative.shape[1] == 500\n",
    "# random sample of tf-idf features for negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append features\n",
    "postings_tfidf_features_only_negative_with_label = pd.concat([\n",
    "    postings_only_negative_with_label,\n",
    "    sample_tf_idf_features_negative], axis=1)\n",
    "\n",
    "postings_tfidf_and_graph_features_only_negative_with_label = pd.merge(postings_tfidf_features_only_negative_with_label, G_features_postings_days_1to3,\n",
    "                                                           on=['Source_User', 'Target_User'], how='left')\n",
    "\n",
    "postings_tfidf_and_graph_features_only_negative_with_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(postings_tfidf_and_graph_features_only_negative_with_label.shape)\n",
    "print(postings_tfidf_and_graph_features_only_positive_with_label.shape)\n",
    "\n",
    "assert postings_tfidf_and_graph_features_only_positive_with_label.shape[1] == postings_tfidf_and_graph_features_only_negative_with_label.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can sample randomly records from the negative samples dataframe, to balance with the postive number of samples, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save negative sample result to file:\n",
    "with open('output/postings_tfidf_and_graph_features_only_negative_days_1to3.pkl', 'wb') as f:\n",
    "    pickle.dump(postings_tfidf_and_graph_features_only_negative_with_label, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load result from file\n",
    "with open('output/postings_tfidf_and_graph_features_only_negative_days_1to3.pkl', 'rb') as f:\n",
    "    postings_tfidf_and_graph_features_only_negative_with_label = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
