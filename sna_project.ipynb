{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Network Analysis - Project sna_topic_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option(\"^display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:20,.4f}'.format)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "The data set is provided by Der Standard, one of the top Austrian newspapers.\n",
    "In the online Standard people can post comments below articles and up/down vote comments.\n",
    "\n",
    "1. There are two files containing all **postings** to articles in May 2019 (due to high amount of data, the postings were split into two files). The respective file also contains additionally meta-data of the postings and articles and some details about the users who composed the postings.\n",
    "\n",
    "2. There are two files containing all **votes** for the postings in point 1 (due to high amount of data, the votes were split into two files). The respective file also contains information whether the vote was negative or positive and some details about the user who did the voting.\n",
    "\n",
    "3. There is one file containing **following and ignoring relationships** among all the users who posted (see point 1) or voted (see point 2) to articles published in May 2019. A following relationship (i.e., the user with the `ID_CommunityIdentity` given in column 1 follows the user with the `ID_CommunityIdentityConnectedTo` given in column 2) is indicated by a “1” in column the `“ID_CommunityConnectionType”`, a ignoring relationship by a “2” in that column (i.e., the user with the `ID_CommunityIdentity` given in column 1 ignores the user with the `ID_CommunityIdentityConnectedTo` given in column 2).\n",
    "\n",
    "There are different entities in the data set: \n",
    "* **Users** - identified by *ID_CommunityIdentity* (or *UserCommunityName*)\n",
    "* **Postings** - identified by *ID_Posting*\n",
    "* **Articles** - identified by *ID_Article*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to datasets:\n",
    "file_postings_1 = 'data/Postings_01052019_15052019.csv'\n",
    "file_postings_2 = 'data/Postings_16052019_31052019.csv'\n",
    "file_votes_1 = 'data/Votes_01052019_15052019.csv'\n",
    "file_votes_2 = 'data/Votes_16052019_31052019.csv'\n",
    "file_following_ignoring = 'data/Following_Ignoring_Relationships_01052019_31052019.csv'\n",
    "\n",
    "output_dir = 'output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   ID_Posting    ID_Posting_Parent  ID_CommunityIdentity                                     PostingHeadline                                                                                      PostingComment         PostingCreatedAt     ID_Article   ArticlePublishingDate                                       ArticleTitle ArticleChannel ArticleRessortName UserCommunityName UserGender            UserCreatedAt\n0  1041073586   1,041,073,234.0000                671476  Das hat gestern bereits der Voggenhuber angeführt!  schieder hatte dem inhaltlich nichts entgegenzusetzen. https://www.youtube.com/watch?v=yiJ-sdjn2Zg  2019-05-01 18:21:15.127  2000102330973  2019-05-01 10:28:57.49  1. Mai in Wien: SPÖ fordert von Strache Rücktritt         Inland           Parteien       Ravenspower        NaN  2018-04-14 13:42:28.470\n1  1041073839   1,041,072,504.0000                566938                                                 NaN                                                      ...und meinen Bezirk bekommst du als Erbe mit.  2019-05-01 18:28:22.040  2000102330973  2019-05-01 10:28:57.49  1. Mai in Wien: SPÖ fordert von Strache Rücktritt         Inland           Parteien        AlphaRomeo          m  2015-08-28 17:07:41.110",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_Posting</th>\n      <th>ID_Posting_Parent</th>\n      <th>ID_CommunityIdentity</th>\n      <th>PostingHeadline</th>\n      <th>PostingComment</th>\n      <th>PostingCreatedAt</th>\n      <th>ID_Article</th>\n      <th>ArticlePublishingDate</th>\n      <th>ArticleTitle</th>\n      <th>ArticleChannel</th>\n      <th>ArticleRessortName</th>\n      <th>UserCommunityName</th>\n      <th>UserGender</th>\n      <th>UserCreatedAt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1041073586</td>\n      <td>1,041,073,234.0000</td>\n      <td>671476</td>\n      <td>Das hat gestern bereits der Voggenhuber angeführt!</td>\n      <td>schieder hatte dem inhaltlich nichts entgegenzusetzen. https://www.youtube.com/watch?v=yiJ-sdjn2Zg</td>\n      <td>2019-05-01 18:21:15.127</td>\n      <td>2000102330973</td>\n      <td>2019-05-01 10:28:57.49</td>\n      <td>1. Mai in Wien: SPÖ fordert von Strache Rücktritt</td>\n      <td>Inland</td>\n      <td>Parteien</td>\n      <td>Ravenspower</td>\n      <td>NaN</td>\n      <td>2018-04-14 13:42:28.470</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1041073839</td>\n      <td>1,041,072,504.0000</td>\n      <td>566938</td>\n      <td>NaN</td>\n      <td>...und meinen Bezirk bekommst du als Erbe mit.</td>\n      <td>2019-05-01 18:28:22.040</td>\n      <td>2000102330973</td>\n      <td>2019-05-01 10:28:57.49</td>\n      <td>1. Mai in Wien: SPÖ fordert von Strache Rücktritt</td>\n      <td>Inland</td>\n      <td>Parteien</td>\n      <td>AlphaRomeo</td>\n      <td>m</td>\n      <td>2015-08-28 17:07:41.110</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 343160 entries, 0 to 343159\n",
      "Data columns (total 14 columns):\n",
      " #   Column                 Non-Null Count   Dtype  \n",
      "---  ------                 --------------   -----  \n",
      " 0   ID_Posting             343160 non-null  int64  \n",
      " 1   ID_Posting_Parent      237112 non-null  float64\n",
      " 2   ID_CommunityIdentity   343160 non-null  int64  \n",
      " 3   PostingHeadline        93344 non-null   object \n",
      " 4   PostingComment         313870 non-null  object \n",
      " 5   PostingCreatedAt       343160 non-null  object \n",
      " 6   ID_Article             343160 non-null  int64  \n",
      " 7   ArticlePublishingDate  343160 non-null  object \n",
      " 8   ArticleTitle           343160 non-null  object \n",
      " 9   ArticleChannel         343160 non-null  object \n",
      " 10  ArticleRessortName     343160 non-null  object \n",
      " 11  UserCommunityName      343159 non-null  object \n",
      " 12  UserGender             256591 non-null  object \n",
      " 13  UserCreatedAt          343160 non-null  object \n",
      "dtypes: float64(1), int64(3), object(10)\n",
      "memory usage: 36.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_postings_1 = pd.read_csv(file_postings_1, sep=';')\n",
    "display(df_postings_1.head(2))\n",
    "df_postings_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   ID_Posting    ID_Posting_Parent  ID_CommunityIdentity                                                        PostingHeadline                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              PostingComment         PostingCreatedAt     ID_Article   ArticlePublishingDate                                               ArticleTitle ArticleChannel   ArticleRessortName UserCommunityName UserGender            UserCreatedAt\n0  1041515171                  NaN                182351  da hat er aber recht ...auch wenn hier nun einige noch immer Naive...  denn Österreich ist von lauter sicheren (EU) Ländern umgeben...die Migranten kommen vorher schon über x sichere Drittländer(=GFK -nur im 1.sicheren Flüchtlingsstatus)... und detto sein Argument zu Griechenland (bei Italien reduziert sich das durch Salvinis Konsequenz gegen sg. \"Retter\" d. kurz vor d. Libyschen Küste wiedermal Schleppern d. Arbeit abnhemen und illegal in d. EU transportieren wollen UND es gab eine EU-Gipfelbschluß im Juni 2018 das auszutrocknen so what? Ö hat nebenbei pro Kopf d. höchsten Zahlen an sg. \"Flüchtlingen\" & auch sogar Anerennungen  (+ was vergessen wird: kumulativ!) - auch hier schon mal geschrieben https://derstandard.at/2000082091102/Was-aus-liberaler-Sicht-fuer-eine-Festung-Europa-spricht Es wird also Zeit  2019-05-16 11:25:39.287  2000103241947  2019-05-16 10:57:22.00  Innenminister Kickl will überhaupt keine Asylanträge mehr         Inland  Integrationspolitik  nadaschauichaber          m  2012-11-25 15:09:03.087\n1  1041515292   1,041,514,595.0000                182351                                                                    NaN                                                                                                                                                                                                                                                                außer von den Naiven die aus 2015 nichts gelernt haben und am Rechtsruck in Europa damit schuld sind - denn keiner will mehr weitere allzuheftiges enstastand inzwischen schon  https://www.sueddeutsche.de/news/panorama/kriminalitaet---duesseldorf-die-ehre-der-familie-lagebild-sieht-104-kriminelle-clans-dpa.urn-newsml-dpa-com-20090101-190514-99-218262 bzw https://www.deutschlandfunk.de/erstes-lagebild-clankriminalitaet-im-kampf-gegen.720.de.html?dram:article_id=448878  wollen wir das auch?  2019-05-16 11:28:44.703  2000103241947  2019-05-16 10:57:22.00  Innenminister Kickl will überhaupt keine Asylanträge mehr         Inland  Integrationspolitik  nadaschauichaber          m  2012-11-25 15:09:03.087",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_Posting</th>\n      <th>ID_Posting_Parent</th>\n      <th>ID_CommunityIdentity</th>\n      <th>PostingHeadline</th>\n      <th>PostingComment</th>\n      <th>PostingCreatedAt</th>\n      <th>ID_Article</th>\n      <th>ArticlePublishingDate</th>\n      <th>ArticleTitle</th>\n      <th>ArticleChannel</th>\n      <th>ArticleRessortName</th>\n      <th>UserCommunityName</th>\n      <th>UserGender</th>\n      <th>UserCreatedAt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1041515171</td>\n      <td>NaN</td>\n      <td>182351</td>\n      <td>da hat er aber recht ...auch wenn hier nun einige noch immer Naive...</td>\n      <td>denn Österreich ist von lauter sicheren (EU) Ländern umgeben...die Migranten kommen vorher schon über x sichere Drittländer(=GFK -nur im 1.sicheren Flüchtlingsstatus)... und detto sein Argument zu Griechenland (bei Italien reduziert sich das durch Salvinis Konsequenz gegen sg. \"Retter\" d. kurz vor d. Libyschen Küste wiedermal Schleppern d. Arbeit abnhemen und illegal in d. EU transportieren wollen UND es gab eine EU-Gipfelbschluß im Juni 2018 das auszutrocknen so what? Ö hat nebenbei pro Kopf d. höchsten Zahlen an sg. \"Flüchtlingen\" &amp; auch sogar Anerennungen  (+ was vergessen wird: kumulativ!) - auch hier schon mal geschrieben https://derstandard.at/2000082091102/Was-aus-liberaler-Sicht-fuer-eine-Festung-Europa-spricht Es wird also Zeit</td>\n      <td>2019-05-16 11:25:39.287</td>\n      <td>2000103241947</td>\n      <td>2019-05-16 10:57:22.00</td>\n      <td>Innenminister Kickl will überhaupt keine Asylanträge mehr</td>\n      <td>Inland</td>\n      <td>Integrationspolitik</td>\n      <td>nadaschauichaber</td>\n      <td>m</td>\n      <td>2012-11-25 15:09:03.087</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1041515292</td>\n      <td>1,041,514,595.0000</td>\n      <td>182351</td>\n      <td>NaN</td>\n      <td>außer von den Naiven die aus 2015 nichts gelernt haben und am Rechtsruck in Europa damit schuld sind - denn keiner will mehr weitere allzuheftiges enstastand inzwischen schon  https://www.sueddeutsche.de/news/panorama/kriminalitaet---duesseldorf-die-ehre-der-familie-lagebild-sieht-104-kriminelle-clans-dpa.urn-newsml-dpa-com-20090101-190514-99-218262 bzw https://www.deutschlandfunk.de/erstes-lagebild-clankriminalitaet-im-kampf-gegen.720.de.html?dram:article_id=448878  wollen wir das auch?</td>\n      <td>2019-05-16 11:28:44.703</td>\n      <td>2000103241947</td>\n      <td>2019-05-16 10:57:22.00</td>\n      <td>Innenminister Kickl will überhaupt keine Asylanträge mehr</td>\n      <td>Inland</td>\n      <td>Integrationspolitik</td>\n      <td>nadaschauichaber</td>\n      <td>m</td>\n      <td>2012-11-25 15:09:03.087</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 395934 entries, 0 to 395933\n",
      "Data columns (total 14 columns):\n",
      " #   Column                 Non-Null Count   Dtype  \n",
      "---  ------                 --------------   -----  \n",
      " 0   ID_Posting             395934 non-null  int64  \n",
      " 1   ID_Posting_Parent      263201 non-null  float64\n",
      " 2   ID_CommunityIdentity   395934 non-null  int64  \n",
      " 3   PostingHeadline        113953 non-null  object \n",
      " 4   PostingComment         363437 non-null  object \n",
      " 5   PostingCreatedAt       395934 non-null  object \n",
      " 6   ID_Article             395934 non-null  int64  \n",
      " 7   ArticlePublishingDate  395934 non-null  object \n",
      " 8   ArticleTitle           395934 non-null  object \n",
      " 9   ArticleChannel         395934 non-null  object \n",
      " 10  ArticleRessortName     395934 non-null  object \n",
      " 11  UserCommunityName      395934 non-null  object \n",
      " 12  UserGender             293078 non-null  object \n",
      " 13  UserCreatedAt          395934 non-null  object \n",
      "dtypes: float64(1), int64(3), object(10)\n",
      "memory usage: 42.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_postings_2 = pd.read_csv(file_postings_2, sep=';')\n",
    "display(df_postings_2.head(2))\n",
    "df_postings_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   ID_CommunityIdentity  ID_Posting  VoteNegative  VotePositive            VoteCreatedAt    UserCommunityName UserGender            UserCreatedAt\n0                675862  1041076570             1             0  2019-05-06 16:47:46.883  Heckscheibenwischer          m  2018-06-26 06:04:30.513\n1                689023  1041076570             1             0  2019-05-01 22:19:06.240      Heinz Fettleber        NaN  2019-03-08 21:23:11.463",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_CommunityIdentity</th>\n      <th>ID_Posting</th>\n      <th>VoteNegative</th>\n      <th>VotePositive</th>\n      <th>VoteCreatedAt</th>\n      <th>UserCommunityName</th>\n      <th>UserGender</th>\n      <th>UserCreatedAt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>675862</td>\n      <td>1041076570</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2019-05-06 16:47:46.883</td>\n      <td>Heckscheibenwischer</td>\n      <td>m</td>\n      <td>2018-06-26 06:04:30.513</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>689023</td>\n      <td>1041076570</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2019-05-01 22:19:06.240</td>\n      <td>Heinz Fettleber</td>\n      <td>NaN</td>\n      <td>2019-03-08 21:23:11.463</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1570737 entries, 0 to 1570736\n",
      "Data columns (total 8 columns):\n",
      " #   Column                Non-Null Count    Dtype \n",
      "---  ------                --------------    ----- \n",
      " 0   ID_CommunityIdentity  1570737 non-null  int64 \n",
      " 1   ID_Posting            1570737 non-null  int64 \n",
      " 2   VoteNegative          1570737 non-null  int64 \n",
      " 3   VotePositive          1570737 non-null  int64 \n",
      " 4   VoteCreatedAt         1570737 non-null  object\n",
      " 5   UserCommunityName     1570731 non-null  object\n",
      " 6   UserGender            1212591 non-null  object\n",
      " 7   UserCreatedAt         1570737 non-null  object\n",
      "dtypes: int64(4), object(4)\n",
      "memory usage: 95.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_votes_1 = pd.read_csv(file_votes_1, sep=';')\n",
    "display(df_votes_1.head(2))\n",
    "df_votes_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   ID_CommunityIdentity  ID_Posting  VoteNegative  VotePositive            VoteCreatedAt UserCommunityName UserGender            UserCreatedAt\n0                571503  1041620947             0             1  2019-05-18 16:34:10.213         vonWeitem          m  2015-11-04 15:45:11.493\n1                178936  1041622392             0             1  2019-05-18 15:57:07.637           phischi          m  2008-09-19 02:02:59.060",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_CommunityIdentity</th>\n      <th>ID_Posting</th>\n      <th>VoteNegative</th>\n      <th>VotePositive</th>\n      <th>VoteCreatedAt</th>\n      <th>UserCommunityName</th>\n      <th>UserGender</th>\n      <th>UserCreatedAt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>571503</td>\n      <td>1041620947</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2019-05-18 16:34:10.213</td>\n      <td>vonWeitem</td>\n      <td>m</td>\n      <td>2015-11-04 15:45:11.493</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>178936</td>\n      <td>1041622392</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2019-05-18 15:57:07.637</td>\n      <td>phischi</td>\n      <td>m</td>\n      <td>2008-09-19 02:02:59.060</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2254247 entries, 0 to 2254246\n",
      "Data columns (total 8 columns):\n",
      " #   Column                Dtype \n",
      "---  ------                ----- \n",
      " 0   ID_CommunityIdentity  int64 \n",
      " 1   ID_Posting            int64 \n",
      " 2   VoteNegative          int64 \n",
      " 3   VotePositive          int64 \n",
      " 4   VoteCreatedAt         object\n",
      " 5   UserCommunityName     object\n",
      " 6   UserGender            object\n",
      " 7   UserCreatedAt         object\n",
      "dtypes: int64(4), object(4)\n",
      "memory usage: 137.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_votes_2 = pd.read_csv(file_votes_2, sep=';')\n",
    "display(df_votes_2.head(2))\n",
    "df_votes_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   ID_CommunityIdentity  ID_CommunityIdentityConnectedTo  ID_CommunityConnectionType\n0                  1778                           246490                           1\n1                  5872                             5872                           1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_CommunityIdentity</th>\n      <th>ID_CommunityIdentityConnectedTo</th>\n      <th>ID_CommunityConnectionType</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1778</td>\n      <td>246490</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5872</td>\n      <td>5872</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 86776 entries, 0 to 86775\n",
      "Data columns (total 3 columns):\n",
      " #   Column                           Non-Null Count  Dtype\n",
      "---  ------                           --------------  -----\n",
      " 0   ID_CommunityIdentity             86776 non-null  int64\n",
      " 1   ID_CommunityIdentityConnectedTo  86776 non-null  int64\n",
      " 2   ID_CommunityConnectionType       86776 non-null  int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 2.0 MB\n"
     ]
    }
   ],
   "source": [
    "df_following_ignoring = pd.read_csv(file_following_ignoring, sep=';')\n",
    "display(df_following_ignoring.head(2))\n",
    "df_following_ignoring.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Concat Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 739094 entries, 0 to 739093\n",
      "Data columns (total 14 columns):\n",
      " #   Column                 Non-Null Count   Dtype         \n",
      "---  ------                 --------------   -----         \n",
      " 0   ID_Posting             739094 non-null  int64         \n",
      " 1   ID_Posting_Parent      500313 non-null  float64       \n",
      " 2   ID_CommunityIdentity   739094 non-null  int64         \n",
      " 3   PostingHeadline        207297 non-null  object        \n",
      " 4   PostingComment         677307 non-null  object        \n",
      " 5   PostingCreatedAt       739094 non-null  datetime64[ns]\n",
      " 6   ID_Article             739094 non-null  int64         \n",
      " 7   ArticlePublishingDate  739094 non-null  datetime64[ns]\n",
      " 8   ArticleTitle           739094 non-null  object        \n",
      " 9   ArticleChannel         739094 non-null  object        \n",
      " 10  ArticleRessortName     739094 non-null  object        \n",
      " 11  UserCommunityName      739093 non-null  object        \n",
      " 12  UserGender             549669 non-null  object        \n",
      " 13  UserCreatedAt          739094 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](3), float64(1), int64(3), object(7)\n",
      "memory usage: 78.9+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "   ID_Posting    ID_Posting_Parent  ID_CommunityIdentity                                     PostingHeadline                                                                                      PostingComment        PostingCreatedAt     ID_Article   ArticlePublishingDate                                       ArticleTitle ArticleChannel ArticleRessortName UserCommunityName UserGender           UserCreatedAt\n0  1041073586   1,041,073,234.0000                671476  Das hat gestern bereits der Voggenhuber angeführt!  schieder hatte dem inhaltlich nichts entgegenzusetzen. https://www.youtube.com/watch?v=yiJ-sdjn2Zg 2019-05-01 18:21:15.127  2000102330973 2019-05-01 10:28:57.490  1. Mai in Wien: SPÖ fordert von Strache Rücktritt         Inland           Parteien       Ravenspower        NaN 2018-04-14 13:42:28.470\n1  1041073839   1,041,072,504.0000                566938                                                 NaN                                                      ...und meinen Bezirk bekommst du als Erbe mit. 2019-05-01 18:28:22.040  2000102330973 2019-05-01 10:28:57.490  1. Mai in Wien: SPÖ fordert von Strache Rücktritt         Inland           Parteien        AlphaRomeo          m 2015-08-28 17:07:41.110",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_Posting</th>\n      <th>ID_Posting_Parent</th>\n      <th>ID_CommunityIdentity</th>\n      <th>PostingHeadline</th>\n      <th>PostingComment</th>\n      <th>PostingCreatedAt</th>\n      <th>ID_Article</th>\n      <th>ArticlePublishingDate</th>\n      <th>ArticleTitle</th>\n      <th>ArticleChannel</th>\n      <th>ArticleRessortName</th>\n      <th>UserCommunityName</th>\n      <th>UserGender</th>\n      <th>UserCreatedAt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1041073586</td>\n      <td>1,041,073,234.0000</td>\n      <td>671476</td>\n      <td>Das hat gestern bereits der Voggenhuber angeführt!</td>\n      <td>schieder hatte dem inhaltlich nichts entgegenzusetzen. https://www.youtube.com/watch?v=yiJ-sdjn2Zg</td>\n      <td>2019-05-01 18:21:15.127</td>\n      <td>2000102330973</td>\n      <td>2019-05-01 10:28:57.490</td>\n      <td>1. Mai in Wien: SPÖ fordert von Strache Rücktritt</td>\n      <td>Inland</td>\n      <td>Parteien</td>\n      <td>Ravenspower</td>\n      <td>NaN</td>\n      <td>2018-04-14 13:42:28.470</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1041073839</td>\n      <td>1,041,072,504.0000</td>\n      <td>566938</td>\n      <td>NaN</td>\n      <td>...und meinen Bezirk bekommst du als Erbe mit.</td>\n      <td>2019-05-01 18:28:22.040</td>\n      <td>2000102330973</td>\n      <td>2019-05-01 10:28:57.490</td>\n      <td>1. Mai in Wien: SPÖ fordert von Strache Rücktritt</td>\n      <td>Inland</td>\n      <td>Parteien</td>\n      <td>AlphaRomeo</td>\n      <td>m</td>\n      <td>2015-08-28 17:07:41.110</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat postings1 and postings2 in order! \n",
    "df_postings = pd.concat([df_postings_1, df_postings_2], ignore_index=True)\n",
    "\n",
    "df_postings[['PostingCreatedAt', 'ArticlePublishingDate', 'UserCreatedAt']] = df_postings[['PostingCreatedAt', 'ArticlePublishingDate', 'UserCreatedAt']].astype('datetime64')\n",
    "\n",
    "df_postings.info()\n",
    "df_postings.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(739094, 14)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO is it necessary to subset the data because it is too large?\n",
    "\n",
    "#df_postings['PostingCreatedAt'].dt.date.head()\n",
    "# df_postings = df_postings[pd.to_datetime(df_postings['PostingCreatedAt'].dt.date) == '2019-05-01']\n",
    "df_postings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with some NLP extractions of text and one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting de-core-news-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.2.0/de_core_news_sm-3.2.0-py3-none-any.whl (19.1 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from de-core-news-sm==3.2.0) (3.2.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (21.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.21.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.3)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (4.0.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\repos\\project-transformers-t10\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# download spacy german language model - used for tokenization, lemmatization of article text (vector data)\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['AutoMobil', 'Bildung', 'Diverses', 'Etat', 'Familie',\n",
      "       'Gesundheit', 'Immobilien', 'Inland', 'International', 'Karriere',\n",
      "       'Kultur', 'Lifestyle', 'Meinung', 'Panorama', 'Reisen', 'Sport',\n",
      "       'User', 'Web', 'Wirtschaft', 'Wissenschaft', 'Zukunft',\n",
      "       'dieStandard'], dtype=object), array(['#MeToo und die Folgen', '1. FC Köln', '1., Innere Stadt',\n",
      "       '10., Favoriten', '11., Simmering', '2., Leopoldstadt',\n",
      "       '20 Jahre Etat', '21., Floridsdorf', '22., Donaustadt',\n",
      "       '6., Mariahilf', '7., Neubau', 'Abtreibung', 'Adipositas',\n",
      "       'Afghanistan', 'Afrika', 'Aktuelles Buch', 'Albanien', 'Albertina',\n",
      "       'Albumkritiken', 'Albumkritiken 2019', 'Algerien', 'Alkohol',\n",
      "       'Allergien', 'Alltag', 'Alternativmedizin', 'Alzheimer & Demenz',\n",
      "       'Amerika', 'Analoge Games ', 'Android', 'András Szigetvari',\n",
      "       'Antisemitismus und Rechtsextremismus', 'Antonio Fian: Dramolette',\n",
      "       'Apple', 'Apps', 'Arbeit & Gesundheit', 'Arbeitsmarkt',\n",
      "       'Arbeitsrecht', 'Arbeitswelten', 'Architektur & Stadt',\n",
      "       'Archäologie', 'Archäologieblog', 'Argentinien', 'Armenien',\n",
      "       'Armut', 'Arzt und Patient', 'Asien & Pazifik', 'Astronomie',\n",
      "       'Atomenergie', 'Audio', 'Australien', 'Austria Wien', 'Auto',\n",
      "       'Automobil', 'Automobil: Umwelt und Technik', 'Außenwirtschaft',\n",
      "       'Axel Springer', 'Baby', 'Baby-Gesundheit', 'Bachmann-Preis ',\n",
      "       'Bahn', 'Ballesterer', 'Banken', 'Barbara Coudenhove-Kalergi',\n",
      "       'Basketball', 'Basketball-Liga', 'Basketball: NBA',\n",
      "       'Bauen & Wohnen', 'Baukunst', 'Beate Hausbichler', 'Belgien',\n",
      "       'Belvedere Wien', 'Bemannte Raumfahrt', 'Bewegung & Fitness',\n",
      "       'Bewerbung', 'Bianca Blei', 'Biathlon', 'Bienensterben',\n",
      "       'Biennale von Venedig', 'Bildende Kunst', 'Bildung',\n",
      "       'Bildung & Integration', 'Bildungsforum', 'Bio-Invasoren',\n",
      "       'Birgit Baumann', 'Bitcoin', 'Bitcoin & Blockchain',\n",
      "       'Blog: Big in Taiwan', 'Blog: Ein Fall für die Wissenschaft',\n",
      "       'Blog: Geschichte Österreichs', 'Blog: Inside Parliament',\n",
      "       'Blog: Stadt, Land, Politik', 'Blog: Tipps für Eltern ',\n",
      "       'Blog: Über die Verhältnisse', 'Borussia Dortmund ', 'Boxen',\n",
      "       'Brand von Notre-Dame', 'Brasilien', 'Brexit', 'Browser',\n",
      "       'Buchneuerscheinungen 2019', 'Bundesliga', 'Bundesländer',\n",
      "       'Bundespräsident', 'Bundestrojaner', 'Burgenland',\n",
      "       'Burgenland Wandertipps', 'Burkina Faso', 'Burnout',\n",
      "       'Bücher zum Thema Essen & Trinken', 'Bühne', 'Cannes Lions',\n",
      "       'Champions League', 'China', 'Christchurch-Attentat',\n",
      "       'Christian Hackl', 'Citroën', 'Community-Blog', 'Conrad Seidl',\n",
      "       'Copyrights', 'Cure - das kritische Gesundheitsmagazin',\n",
      "       'DFB-Pokal', 'DTM', 'Da muss man durch', 'Darts',\n",
      "       'Daten und Interaktiv', 'Datenschutz', 'David Rennert', 'Debatte',\n",
      "       'Debatten im Netz', 'Design & Interieur', 'Deutsche Bundesliga',\n",
      "       'Deutsche Wirtschaft', 'Deutscher Immobilienmarkt', 'Deutschland',\n",
      "       'Deutschland-Chronik', 'Diabetes', 'Die Uhren der Woche',\n",
      "       'Digitale Zukunft', 'Dominik Kamalzadeh', 'Donaufestival ',\n",
      "       'Donauinselfest', 'Doping', 'Dänemark', 'E-Sport',\n",
      "       'EBEL Eishockey-Liga', 'EU', 'EU-Wahl 2019',\n",
      "       'EU-Wahl 2019 in Österreich: Wahlergebnisse und -gr', 'Ebola',\n",
      "       'Edition Zukunft', 'Eigenbau', 'Einkommen',\n",
      "       'Einserkastl Christoph Winder', 'Einserkastl Gudrun Harrer',\n",
      "       'Einserkastl Hans Rauscher', 'Einserkastl Renate Graber',\n",
      "       'Eishockey', 'Eishockey-WM 2019', 'Eishockey: NHL',\n",
      "       'Empfehlungen aus der WebStandard-Redaktion', 'Energiemarkt',\n",
      "       'Erdbeben', 'Eric Frey', 'Essen & Trinken', 'Essstörungen ',\n",
      "       'Estland', 'Etat', 'Etat-Quiz', 'Euro 2020: Qualifikation',\n",
      "       'Eurofighter', 'Europa', 'Europa League', 'Europa Reisen',\n",
      "       'Europa spricht', 'Europäische Kommission', 'FA-Cup',\n",
      "       'FC Barcelona', 'FC Bayern München', 'FIFA', 'FPÖ', 'FSME/Zecken',\n",
      "       'Fabian Schmid', 'Facebook', 'Fachhochschule',\n",
      "       'Fahrrad-Kolumne: Radkasten', 'Familie', 'Familie & Gesundheit',\n",
      "       'Familienrat', 'Familienrecht', 'Fernsehkritik: TV-Tagebuch',\n",
      "       'Film', 'Filmfestival Cannes', 'Filmforum', 'Filmkritik',\n",
      "       'Finanzen & Börse', 'Florian Scheuba', 'Flucht und Politik',\n",
      "       'Flugzeugkatastrophen', 'Flüchtlinge', 'Flüchtlinge in Österreich',\n",
      "       'Foodblog: Neuens aus der Küche', 'Ford', 'Formel 1',\n",
      "       'Forschung Spezial', 'Forschungspolitik', 'Frankreich',\n",
      "       'Frankreich Urlaub', 'Frauen-Fußball', 'French Open', 'Fußball',\n",
      "       'Fußball International', 'Game of Thrones', 'GamePolitics',\n",
      "       'Games', 'Games-Blog: Berufsspieler', 'Games-Rezensionen',\n",
      "       'Gaming-Hardware', 'Garten', 'Gehalt',\n",
      "       'Geheimdokumente: Wikileaks ', 'Geistesblitz', 'Geld',\n",
      "       'Gemišt - Olivera Stajic', 'Genetik & Molekularbiologie',\n",
      "       'Gentechnik und Landwirtschaft', 'Genussforum', 'Gerald Schubert',\n",
      "       'Gerichtsreportagen', 'Geschlechterpolitik',\n",
      "       'Gesehen: \"Game of Thrones\"', 'Gesellschaft', 'Gesunde Ernährung',\n",
      "       'Gesunde Geschichten', 'Gesundheitspolitik', 'Gianluca Wallisch',\n",
      "       \"Giro d'Italia\", 'Golf', 'Griechenland', 'Großbritannien',\n",
      "       'Gruß aus der Küche', 'Grüne', 'Guatemala',\n",
      "       'Gudrun Harrer: Analysen & Kommentare', 'Gute Frage',\n",
      "       'Gutenberg-Galaxis', 'Günter Traxler',\n",
      "       'Günter Traxler Kolumne: Blattsalat', 'Günther Oswald',\n",
      "       'Günther Strobl', 'HIV/Aids', 'Handball', 'Handel',\n",
      "       'Hans Rauscher', 'Hardware', 'Hartberg', 'Haustiere',\n",
      "       'Heer & Zivildienst', 'Heute', 'Hinter der Fassade',\n",
      "       'Hochschul-Personal', 'Hochschule',\n",
      "       'Homosexualität und Homophobie', 'Hyundai', 'ISS',\n",
      "       'IT & Developer', 'IT Start-Up', 'IT-Business', 'IT-Security',\n",
      "       'Idealbesetzung', 'Immo-Mitreden', 'Immo-Wirtschaft',\n",
      "       'Immo: Bundesländer', 'Immobilien-Deals', 'Impfungen', 'Indien',\n",
      "       'Indonesien', 'Infektionen', 'Inklusion von Behinderten', 'Inland',\n",
      "       'Innovationen', 'Insolvenzen', 'Integrationspolitik',\n",
      "       'International', 'Internet',\n",
      "       'Interviewserie: Fachdidaktik kontrovers', 'Irak', 'Iran',\n",
      "       'Irene Brickner', 'Irland', 'Israel', 'Italien', 'Italien Urlaub',\n",
      "       'Japan', 'Jemen', 'Jihadismus', 'Job & Karriere', 'Jobwelten',\n",
      "       'Julya Rabinowich', 'Junge-Akademie-Blog', 'Kabarett',\n",
      "       'Karin Riss', 'Karriere', 'Kia', 'Kim Son Hoang ',\n",
      "       'Kinder- und Jugendgesundheit ', 'Kinderbücher', 'Kirche', 'Klima',\n",
      "       'Klimawandel', 'Knecht mich!', 'Koalition', 'Kochvideos',\n",
      "       'Kolumbien', 'Kolumne: Das beste Stück',\n",
      "       'Kolumne: Der letzte Schrei', 'Kolumne: Mann könnte ja mal ...',\n",
      "       'Kolumne: Pro & Kontra', 'Kolumne: Realitäten', 'Kommentare',\n",
      "       'Kommentare Pro und Kontra', 'Kommentare der anderen', 'Konsolen',\n",
      "       'Konzerte', 'Kosovo ', 'Krankenhaus Nord', 'Krankheit', 'Krebs',\n",
      "       'Kriminalität', 'Kroatien Urlaub', 'Kronen Zeitung', 'Kuba',\n",
      "       'Kultur', 'KulturGlosse', 'Kulturpolitik', 'Kulturpolitik Wien',\n",
      "       'Kulturpolitik der Bundesländer', 'Kunst und Kultur', 'Kunstmarkt',\n",
      "       'Kärnten', 'Kärnten Urlaub', 'Königshäuser', 'LASK',\n",
      "       'LeadershipStandard', 'Leber', 'Leichtathletik ', 'Leopold Museum',\n",
      "       'Leopold Stefan ', 'Libanon', 'Libyen', 'Liebe & Sex ',\n",
      "       'Liebesforum', 'Life Ball', 'Linux/Unix', 'Lisa Nimmervoll',\n",
      "       'Liste Jetzt', 'Litauen', 'Literatur', 'Literaturforum',\n",
      "       'Literaturpreise', 'Lobbying & Korruption', 'Lokale im 1. Bezirk',\n",
      "       'Lokale im 2. und 20. Bezirk', 'Lokale im 9. Bezirk',\n",
      "       'Lokale im Burgenland', 'Lokale in den Bezirken 13, 14 und 15',\n",
      "       'Luftfahrt', 'Lunge', 'Lustprinzip', 'Luxusimmobilien',\n",
      "       'Magen-Darm', 'Management', 'Management Standard',\n",
      "       'Manuela Honsig-Erlenburg', 'Maria Sterkl', 'Marietta Adenberger',\n",
      "       'Martin Kotynek', 'Medien', 'Medien und Politik',\n",
      "       'Mediengruppe Österreich', 'Medienpreise', 'Mehr Lifestyle',\n",
      "       'Mehr Sport', 'Meistergruppe', 'Mensch', 'Mercedes-Benz',\n",
      "       'Michael Simoner', 'Michael Völker', 'Microsoft', 'Mieten',\n",
      "       'Minibar', 'Missbrauch', 'Mitreden', 'Mitreden: Freundschaft',\n",
      "       'Mitreden: Leben mit Kind', 'Mitreden: Rund ums Auto',\n",
      "       'Mobile-Games', 'Mobilfunker', 'Mobilität in Wien',\n",
      "       'Mode & Kosmetik', 'Montagsfrage', 'Moto GP', 'Motorrad',\n",
      "       'Motorsport ', 'Multiple Sklerose', 'Musik', 'Musikfestivals',\n",
      "       'Myanmar', 'Nachrufe', 'Nachrufe 2019', 'Naher Osten',\n",
      "       'Nahost-Konflikt', 'Nationalrat', 'Nationalratswahl 2017',\n",
      "       'Nationalsozialismus', 'Nationalteam', 'Nato', 'Natur',\n",
      "       'Naturkatastrophen', 'Neos', 'Netflix', 'Netzpolitik',\n",
      "       'Neuseeland', 'Nicaragua', 'Niederlande', 'Niederösterreich',\n",
      "       'Niederösterreich Urlaub', 'Niere', 'Nigeria', 'Niki Lauda',\n",
      "       'Nina Weißensteiner', 'Nora Laufer', 'Nordische Ski-WM 2019',\n",
      "       'Nordische WM 2019: Doping', 'Nordkorea', 'Nordmazedonien',\n",
      "       'Norwegen', 'Notenbanken', 'ORF', 'ORF-Programm', 'Oberösterreich',\n",
      "       'Oberösterreich Urlaub', 'OeNB', 'Off-Topic', 'Online-Handel',\n",
      "       'Orthopädie', 'Ozeanien Reisen', 'Pakistan', 'Paläontologie',\n",
      "       'Panama', 'Panorama', 'Parlament', 'Parteien', 'Paul Lendvai',\n",
      "       'Pensionen', 'Personal Moves', 'Petra Stuiber', 'Peugeot',\n",
      "       'Philippinen', 'Phänomen Hass', 'Pictotop: Der Comic-Blog',\n",
      "       'Podcast Edition Zukunft', 'Podcast-Test', 'Polen',\n",
      "       'Politikwissenschaftsblog', 'Politische Umfragen', 'Polizei',\n",
      "       'Portugal', 'Premier League', 'Presse- und Meinungsfreiheit',\n",
      "       'Presserat', 'Primera Division', 'ProSiebensat1Puls4',\n",
      "       'Produkt der Woche', 'Prozessor', 'Prävention', 'Psyche',\n",
      "       'Psychische Erkrankungen', 'Psychologie & Verhalten', 'Puls 4',\n",
      "       'QWIEN-Blog: Queere Geschichte', 'Qualifikationsgruppe',\n",
      "       'Quiz: Mitraten', 'Quizblog', 'Rad', 'Radio', 'Radsport',\n",
      "       'Rainer Schüller', 'Rapid Wien', 'Rauchen', 'Raum', 'Rauschmittel',\n",
      "       'Real Madrid', 'Rechtsextremismus', 'Red Bull Salzburg',\n",
      "       'Regionalliga', 'Reiseforum', 'Reisen aktuell',\n",
      "       'Reisen nach Vorderasien', 'Reitsport', 'Religion & Politik',\n",
      "       'Retro Games', 'Ronald Pohls Mittel-Alter', 'Rondomobil',\n",
      "       'Rosa Winkler-Hermaden', 'Rumänien', 'Rundfunkgebühr', 'Russland',\n",
      "       'Rüstung', 'SCR Altach', 'SPÖ', 'STANDARD-Zyklus',\n",
      "       'SV Mattersburg', 'Sachpolitik', 'Salzburg',\n",
      "       'Salzburger Festspiele', 'Sandra Weiss', 'Saudi-Arabien',\n",
      "       'Schachingers Daneben Gehen', 'Schlafen & Schlafstörungen ',\n",
      "       'Schul-Politik', 'Schule', 'Schule und Kindergarten',\n",
      "       'Schwangerschaft & Kinderwunsch', 'Schweden', 'Schweiz',\n",
      "       'Schwimmen', 'Science Fiction & Fantasy', 'Sebastian Borger',\n",
      "       'Sebastian Fellner',\n",
      "       'Semesterfrage der Uni Wien: \"Wie werden wir morgen', 'Serie A',\n",
      "       'Serienforum', 'Serienreif', 'Sexualität', 'Sicherheitspolitik',\n",
      "       'Siegfried Lützow', 'Simbabwe', 'Ski Alpin: Herren-Weltcup',\n",
      "       'Ski-Weltcup', 'Skispringen', 'Sky', 'Slowakei', 'Smart Home',\n",
      "       'Smartphones', 'Snooker-WM', 'Social Media', 'Somalia',\n",
      "       'Song Contest ', 'Song Contest 2019', 'Song-Contest-Blog',\n",
      "       'Soziales', 'Sozialpolitik & Armut', 'Spanien',\n",
      "       'Sport und Politik', 'Sportfeature', 'Sportler: Das wurde aus ...',\n",
      "       'Sportmonolog', 'Sportpolitik', 'Sri Lanka', 'Staat & Justiz',\n",
      "       'Staatsoper Wien', 'Stadtentwicklung', 'Standardabweichung',\n",
      "       'Start up', 'Stephan Hilpold', 'Steuern', 'Streaming und TV',\n",
      "       'Sturm Graz', 'Styria', 'Städtereisen', 'Suchmaschinen', 'Sucht',\n",
      "       'Sudan', 'Syrien', 'Südafrika', 'TV', 'TV-Programm: Switchlist',\n",
      "       'TV-Serien', 'Tadschikistan', 'Tanz/Performance', 'Tatort',\n",
      "       'Taxidienst Uber', 'Technik', 'Telekom', 'Telekom und Buwog',\n",
      "       'Telekom-Politik', 'Tennis', 'Thailand', 'Theater',\n",
      "       'Theater an der Wien', 'Theater in der Josefstadt',\n",
      "       'Therapie & Medikamente', 'Thomas Mayer', 'Tiere', 'Tierethik',\n",
      "       'Tiergärten', 'Tirol', 'Tourismus', 'Transfermarkt',\n",
      "       'Transport & Logistik', 'Tretlager', 'Tschechische Republik',\n",
      "       'Tutscheks Zeitreiseblog', 'Twitter', 'Türkei', 'UNO', 'USA',\n",
      "       'USA-EssBar', 'Uhren & Schmuck', 'Ukraine',\n",
      "       'Umfragen und Demografischer Wandel',\n",
      "       'Umwelt, Landwirtschaft & Klima', 'Umweltmedizin', 'Ungarn', 'Uni',\n",
      "       'Uni & Soziales', 'Unternehmen', 'Unternehmensgründung & Start up',\n",
      "       'Unterricht', 'Urlaub in Nordeuropa', 'Urlaub in Südeuropa',\n",
      "       'Urlaub in Westeuropa', 'Userartikel', 'VGN ', 'Venezuela',\n",
      "       'Verfassungsschutz', 'Verkehr', 'Verkehr & Kosten',\n",
      "       'Vernachlässigte Krisen', 'Verteilung', 'Video', 'Vorarlberg',\n",
      "       'WM 2022 in Katar', 'Wahlen in Deutschland', 'Wahlen in den USA',\n",
      "       'Was ist los am Wochenende?', 'Was ist los in Wien?',\n",
      "       'Web-Analyse', 'Web-Quiz', 'Webanalyse ÖWA', 'Webmix', 'Webtalk',\n",
      "       'Wein', 'Weiterbildung', 'Welt', 'Welt-Chronik', 'Welthandel',\n",
      "       'Werbepreise', 'Werbung', 'Wetternachrichten', 'Whatsapp-Updates',\n",
      "       'Wien', 'Wien Wandertipps', 'Wiener Festwochen ',\n",
      "       'Wiener Musikverein', 'Wiener Politik', 'Wiener Rathaus',\n",
      "       'Wikileaks', 'Wildtiere', 'Wintersport', 'Wirtschaftspolitik',\n",
      "       'Wirtschaftsrecht', 'Wo Migration beginnt',\n",
      "       'Wohnbau in Österreich', 'Wohnen in und um Wien', 'Wohngespräch',\n",
      "       'Wohnrecht', 'Wolfsberger AC', 'Zeit',\n",
      "       'Zentralafrikanische Republik', 'Zukunftsbranche Elektrotechnik',\n",
      "       'Zweite Liga', 'Zweite deutsche Liga', 'Zypern', 'Zähne',\n",
      "       'alles gut? Weltverbessern für Anfänger', 'Ägypten', 'ÖBB',\n",
      "       'ÖFB-Cup', 'ÖFB-Nachwuchs', 'ÖH-Wahl 2019', 'ÖVP',\n",
      "       'Öffentlicher Dienst', 'Öffis', 'Ökologie', 'Ökonomieblog',\n",
      "       'Österreich', 'Österreich und EU', 'Überwachung'], dtype=object)]\n",
      "(739094, 711)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# encode article channel and article ressort name with one hot encoding\n",
    " \n",
    "# ArticleChannel\tArticleRessortName\n",
    "one_hot_encoder_article_channel = OneHotEncoder(handle_unknown='ignore').fit(df_postings[['ArticleChannel', 'ArticleRessortName']])\n",
    "\n",
    "print(one_hot_encoder_article_channel.categories_)\n",
    "\n",
    "transformed_channel_resort = one_hot_encoder_article_channel.transform(df_postings[['ArticleChannel', 'ArticleRessortName']])\n",
    "print(transformed_channel_resort.shape)\n",
    "display(transformed_channel_resort.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step for creating 'output/combinedPostingText.spacy' is usually not necessary. It takes around 64 minutes and you can just use my file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "df_postings['CombinedArticlePostingText'] = df_postings['PostingHeadline'].fillna('') + ' ' + df_postings['PostingComment'].fillna('') + ' ' + df_postings['ArticleTitle'].fillna('')\n",
    "\n",
    "doc_bin = DocBin(attrs=[\"LEMMA\", \"ENT_IOB\", \"ENT_TYPE\"], store_user_data=True)\n",
    "\n",
    "for doc in nlp.pipe(df_postings['CombinedArticlePostingText'].str.lower()):\n",
    "    #print(repr(doc))\n",
    "    doc_bin.add(doc)\n",
    "\n",
    "#bytes_data = doc_bin.to_bytes()\n",
    "#doc_bin = DocBin().from_bytes(bytes_data)\n",
    "\n",
    "file_name_spacy = 'output/combinedPostingText.spacy'\n",
    "doc_bin.to_disk(file_name_spacy)\n",
    "\n",
    "# save tokenized spacy Doc objects to file\n",
    "# the file contains the combined text of posting headline, posting comment and article title in a tokenized form\n",
    "# and also meta data about a token, like its lemma form, whether a token is a punctuation, or bracket etc.\n",
    "# processing took around 64 minutes on the whole df_postings for me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "739094\n",
      "739094\n",
      "['gestern', 'voggenhuber', 'anführen', 'schieder', 'inhaltlich', 'entgegensetzen', '1.', 'mai', 'wien', 'spö', 'fordern', 'strache', 'rücktritt']\n",
      "['sofortig', 'stopp', 'subventionen', 'städter', 'steuergeld', 'wundern', 'landbewohner', 'fordern', 'leben', 'stadt', 'mein', 'land', 'aussehen', 'ruhig', 'schlafen', 'garantieren', 'köstinger', 'verfehlen', 'klimaziele', 'kosten', 'steuerentlastung']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "doc_bin = DocBin().from_disk(file_name_spacy)\n",
    "\n",
    "docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "#print(docs)\n",
    "print(len(docs))\n",
    "tokenized_lemmatized_texts = [[token.lemma_ for token in doc \n",
    "                               if not token.is_stop and not token.is_punct and not token.is_space \n",
    "                               and not token.like_url and not token.like_email and not token.is_quote \n",
    "                               and not token.is_currency and not token.is_bracket and not token.is_quote\n",
    "                               and not token.is_left_punct and not token.is_right_punct and not token.is_bracket]\n",
    "                               for doc in docs]\n",
    "print(len(tokenized_lemmatized_texts))\n",
    "print(tokenized_lemmatized_texts[0])\n",
    "print(tokenized_lemmatized_texts[100])\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), lowercase=False, tokenizer=lambda x: x, max_features=3000)\n",
    "vectorizer = vectorizer.fit(tokenized_lemmatized_texts)\n",
    "#print(list(vectorizer.get_feature_names_out()))\n",
    "\n",
    "# TODO train test split!\n",
    "# only fit on train data and not on test data!\n",
    "\n",
    "# text_vectorized = vectorizer.transform(tokenized_lemmatized_texts)\n",
    "#print(text_vectorized.toarray().shape)\n",
    "#display(text_vectorized.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a check to see which tf-idf features were extracted:\n",
    "with open('output/feature_names.txt', 'w') as f:\n",
    "    for item in vectorizer.get_feature_names_out():\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation 1: User_A commented/posted to post of User_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Edge relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of iterating over the dataframe rows to create a edge list, I use a join, which is way faster on the whole dataset. <br>\n",
    "But, by default `merge` changes the sort order, so you have to sort by the original dataframe index!<br>\n",
    "The following code creates a dataframe for posting source user and target posting user. It has the same shape as the original dataframe `df_postings`.\n",
    "So, target posting user may be `NaN`, due to left join and preserving all observations in the posting dataframe.\n",
    "One could filter the Target_User column by not `NaN` to get only relations between users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(739094, 15)\n",
      "(739094, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "       ID_Posting        PostingCreatedAt  Source_User          Target_User\nindex                                                                      \n0      1041073586 2019-05-01 18:21:15.127       671476         233,191.0000\n1      1041073839 2019-05-01 18:28:22.040       566938         640,123.0000\n2      1041073872 2019-05-01 18:29:05.533       669286         680,772.0000\n3      1041080734 2019-05-01 22:37:56.010       671476          51,817.0000\n4      1041080828 2019-05-01 22:42:06.310       671476                  NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_Posting</th>\n      <th>PostingCreatedAt</th>\n      <th>Source_User</th>\n      <th>Target_User</th>\n    </tr>\n    <tr>\n      <th>index</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1041073586</td>\n      <td>2019-05-01 18:21:15.127</td>\n      <td>671476</td>\n      <td>233,191.0000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1041073839</td>\n      <td>2019-05-01 18:28:22.040</td>\n      <td>566938</td>\n      <td>640,123.0000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1041073872</td>\n      <td>2019-05-01 18:29:05.533</td>\n      <td>669286</td>\n      <td>680,772.0000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1041080734</td>\n      <td>2019-05-01 22:37:56.010</td>\n      <td>671476</td>\n      <td>51,817.0000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1041080828</td>\n      <td>2019-05-01 22:42:06.310</td>\n      <td>671476</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "postings_user_commented_relation_df = df_postings.reset_index()[['index', 'ID_Posting', 'ID_CommunityIdentity', 'PostingCreatedAt', 'ID_Posting_Parent']].merge(\n",
    "    df_postings[['ID_Posting', 'ID_CommunityIdentity']], \n",
    "    left_on='ID_Posting_Parent', right_on='ID_Posting',\n",
    "    suffixes=('', '_parent'), how='left', sort=False\n",
    ")[['index', 'ID_Posting', 'PostingCreatedAt', 'ID_CommunityIdentity', 'ID_CommunityIdentity_parent']].sort_values(by='index')\n",
    "\n",
    "postings_user_commented_relation_df.set_index('index', inplace=True) # index may be useful for selection?\n",
    "# postings_user_commented_relation_df.set_index(['ID_Posting', 'PostingCreatedAt'], inplace=True) # index may be useful for selection?\n",
    "postings_user_commented_relation_df.rename(columns={'ID_CommunityIdentity': 'Source_User', 'ID_CommunityIdentity_parent': 'Target_User'}, inplace=True)\n",
    "\n",
    "# postings_user_commented_relation_df.drop('index', axis='columns', inplace=True) # index is ascending as original df_postings, index column not needed anymore\n",
    "\n",
    "# SAME number of rows and SAME order as original df_postings:\n",
    "print(df_postings.shape)\n",
    "print(postings_user_commented_relation_df.shape)\n",
    "\n",
    "display(postings_user_commented_relation_df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to a file takes longer than just obtaining the edge list.\n",
    "postings_user_commented_relation_df.to_csv(\"output/postings_user_commented_relation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(739094, 4)\n",
      "(500312, 4)\n"
     ]
    }
   ],
   "source": [
    "postings_user_commented_relation_nonan_df = postings_user_commented_relation_df.dropna()\n",
    "# check number of rows after dropping NAN\n",
    "print(postings_user_commented_relation_df.shape)\n",
    "print(postings_user_commented_relation_nonan_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undirected Graph measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see for example:\n",
    "* https://networkx.org/documentation/stable/reference/algorithms/link_prediction.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create undirected graph:\n",
    "G_postings_commented_undirected = nx.from_pandas_edgelist(postings_user_commented_relation_nonan_df,\n",
    "                                                          source='Source_User',\n",
    "                                                          target='Target_User',\n",
    "                                                          create_using=nx.Graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 21761 nodes and 337257 edges\n"
     ]
    }
   ],
   "source": [
    "print(nx.info(G_postings_commented_undirected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.link_prediction.jaccard_coefficient.html#networkx.algorithms.link_prediction.jaccard_coefficient\n",
    "G_postings_jaccard_coefs_iter = nx.jaccard_coefficient(G_postings_commented_undirected)\n",
    "#for u, v, p in G_postings_jaccard_coefs:\n",
    "    #print(f\"({u}, {v}) -> {p:.8f}\")\n",
    "\n",
    "# returns iterator\n",
    "# G_postings_jaccard_coefs = list(G_postings_jaccard_coefs_iter)\n",
    "# print(len(G_postings_jaccard_coefs))\n",
    "# display(G_postings_jaccard_coefs[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_20784/3224929055.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# this tooks 200 minutes for me...\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mG_postings_jaccard_coefs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mr\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mr\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjaccard_coefficient\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mG_postings_commented_undirected\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# filter out those coefs with value zero, can be derived\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mc:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages\\networkx\\algorithms\\link_prediction.py\u001B[0m in \u001B[0;36m<genexpr>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     38\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mebunch\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     39\u001B[0m         \u001B[0mebunch\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnon_edges\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mG\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 40\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mu\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mu\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mv\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mu\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mv\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mebunch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     41\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\repos\\project-transformers-t10\\.venv\\lib\\site-packages\\networkx\\classes\\function.py\u001B[0m in \u001B[0;36mnon_edges\u001B[1;34m(graph)\u001B[0m\n\u001B[0;32m    919\u001B[0m         \u001B[1;32mwhile\u001B[0m \u001B[0mnodes\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    920\u001B[0m             \u001B[0mu\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnodes\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpop\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 921\u001B[1;33m             \u001B[1;32mfor\u001B[0m \u001B[0mv\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mnodes\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgraph\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mu\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    922\u001B[0m                 \u001B[1;32myield\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mu\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mv\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    923\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mMemoryError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# this tooks 200 minutes for me...\n",
    "G_postings_jaccard_coefs = list(filter(lambda r: r[2] != 0, nx.jaccard_coefficient(G_postings_commented_undirected))) # filter out those coefs with value zero, can be derived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adamic-Adar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.link_prediction.adamic_adar_index.html#networkx.algorithms.link_prediction.adamic_adar_index\n",
    "\n",
    "G_postings_adamic_adar_iter = nx.adamic_adar_index(G_postings_commented_undirected)\n",
    "#for u, v, p in G_postings_adamic_adar_iter:\n",
    "    #print(f\"({u}, {v}) -> {p:.8f}\")\n",
    "# returns iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this tooks 205 minutes for me...\n",
    "G_postings_adamic_adar = list(filter(lambda r: r[2] != 0, nx.adamic_adar_index(G_postings_commented_undirected)))   # filter out those coefs with value zero, can be derived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preferential attachment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.link_prediction.preferential_attachment.html#networkx.algorithms.link_prediction.preferential_attachment\n",
    "\n",
    "G_postings_preferential_attachment_iter = nx.preferential_attachment(G_postings_commented_undirected)\n",
    "#for u, v, p in G_postings_preferential_attachment_iter:\n",
    "    #print(f\"({u}, {v}) -> {p:.8f}\")\n",
    "# returns iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_postings_preferential_attachment = list(filter(lambda r: r[2] != 0, nx.preferential_attachment(G_postings_commented_undirected)))   # filter out those coefs with value zero, can be derived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_postings_jaccard_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_postings_adamic_adar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('output/G_postings_jaccard_coefs.pkl', 'wb') as f:\n",
    "    pickle.dump(G_postings_jaccard_coefs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('output/G_postings_adamic_adar.pkl', 'wb') as f:\n",
    "    pickle.dump(G_postings_adamic_adar, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('output/G_postings_preferential_attachment.pkl', 'wb') as f:\n",
    "    pickle.dump(G_postings_preferential_attachment, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#with open('output/G_postings_jaccard_coefs.pkl', 'rb') as f:\n",
    "    #test123 = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directed Graph (DiGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi directed graph:\n",
    "\n",
    "\n",
    "postings_user_commented_relation_nonan_count_df = postings_user_commented_relation_nonan_df[['ID_Posting', 'Source_User', 'Target_User']].groupby(['Source_User', 'Target_User'])['ID_Posting'].count().reset_index(name=\"weight\")\n",
    "\n",
    "G_postings_commented_directed = nx.from_pandas_edgelist(postings_user_commented_relation_nonan_count_df,\n",
    "                                                          source='Source_User',\n",
    "                                                          target='Target_User',\n",
    "                                                          edge_attr='weight',\n",
    "                                                          create_using=nx.DiGraph())\n",
    "display(postings_user_commented_relation_nonan_count_df.sort_values('weight'))\n",
    "print(nx.info(G_postings_commented_directed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if edge data is named 'weight' - is default edge data key to use as weight:\n",
    "print(G_postings_commented_directed.get_edge_data(635206, 51140))\n",
    "print(G_postings_commented_directed.get_edge_data(51140, 635206))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the PageRank of the nodes in the graph.\n",
    "PageRank computes a ranking of the nodes in the graph G based on the structure of the incoming links. It was originally designed as an algorithm to rank web pages.\n",
    "\n",
    "Edge data attribute 'weight' is default edge data key to use as weight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.link_analysis.pagerank_alg.pagerank.html#networkx.algorithms.link_analysis.pagerank_alg.pagerank\n",
    "# Edge data key to use as weight is by default 'weight'.\n",
    "\n",
    "G_postings_pagerank = nx.pagerank(G_postings_commented_directed)\n",
    "# Returns the PageRank of the nodes in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expecting length of dictionary to be number of nodes in graph\n",
    "print(len(G_postings_pagerank))\n",
    "G_postings_pagerank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HITS hubs and authorities values for nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.link_analysis.hits_alg.hits.html#networkx.algorithms.link_analysis.hits_alg.hits\n",
    "\n",
    "G_postings_hits = nx.hits(G_postings_commented_directed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Katz weighted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.katz_centrality.html#networkx.algorithms.centrality.katz_centrality\n",
    "\n",
    "G_postings_katz_weighted = nx.katz_centrality(G_postings_commented_directed, max_iter=10000, tol=1.0e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}