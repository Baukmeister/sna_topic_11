{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Network Analysis - Project sna_topic_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import compress\n",
    "import pickle\n",
    "from itertools import compress, product\n",
    "import datetime\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option(\"^display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:20,.4f}'.format)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "The data set is provided by Der Standard, one of the top Austrian newspapers.\n",
    "In the online Standard people can post comments below articles and up/down vote comments.\n",
    "\n",
    "1. There are two files containing all **postings** to articles in May 2019 (due to high amount of data, the postings were split into two files). The respective file also contains additionally meta-data of the postings and articles and some details about the users who composed the postings.\n",
    "\n",
    "2. There are two files containing all **votes** for the postings in point 1 (due to high amount of data, the votes were split into two files). The respective file also contains information whether the vote was negative or positive and some details about the user who did the voting.\n",
    "\n",
    "3. There is one file containing **following and ignoring relationships** among all the users who posted (see point 1) or voted (see point 2) to articles published in May 2019. A following relationship (i.e., the user with the `ID_CommunityIdentity` given in column 1 follows the user with the `ID_CommunityIdentityConnectedTo` given in column 2) is indicated by a “1” in column the `“ID_CommunityConnectionType”`, a ignoring relationship by a “2” in that column (i.e., the user with the `ID_CommunityIdentity` given in column 1 ignores the user with the `ID_CommunityIdentityConnectedTo` given in column 2).\n",
    "\n",
    "There are different entities in the data set: \n",
    "* **Users** - identified by *ID_CommunityIdentity* (or *UserCommunityName*)\n",
    "* **Postings** - identified by *ID_Posting*\n",
    "* **Articles** - identified by *ID_Article*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to datasets:\n",
    "file_postings_1 = 'data/Postings_01052019_15052019.csv'\n",
    "file_postings_2 = 'data/Postings_16052019_31052019.csv'\n",
    "file_votes_1 = 'data/Votes_01052019_15052019.csv'\n",
    "file_votes_2 = 'data/Votes_16052019_31052019.csv'\n",
    "file_following_ignoring = 'data/Following_Ignoring_Relationships_01052019_31052019.csv'\n",
    "\n",
    "output_dir = 'output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Posting</th>\n",
       "      <th>ID_Posting_Parent</th>\n",
       "      <th>ID_CommunityIdentity</th>\n",
       "      <th>PostingHeadline</th>\n",
       "      <th>PostingComment</th>\n",
       "      <th>PostingCreatedAt</th>\n",
       "      <th>ID_Article</th>\n",
       "      <th>ArticlePublishingDate</th>\n",
       "      <th>ArticleTitle</th>\n",
       "      <th>ArticleChannel</th>\n",
       "      <th>ArticleRessortName</th>\n",
       "      <th>UserCommunityName</th>\n",
       "      <th>UserGender</th>\n",
       "      <th>UserCreatedAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1041073586</td>\n",
       "      <td>1,041,073,234.0000</td>\n",
       "      <td>671476</td>\n",
       "      <td>Das hat gestern bereits der Voggenhuber angeführt!</td>\n",
       "      <td>schieder hatte dem inhaltlich nichts entgegenzusetzen. https://www.youtube.com/watch?v=yiJ-sdjn2Zg</td>\n",
       "      <td>2019-05-01 18:21:15.127</td>\n",
       "      <td>2000102330973</td>\n",
       "      <td>2019-05-01 10:28:57.49</td>\n",
       "      <td>1. Mai in Wien: SPÖ fordert von Strache Rücktritt</td>\n",
       "      <td>Inland</td>\n",
       "      <td>Parteien</td>\n",
       "      <td>Ravenspower</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-04-14 13:42:28.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1041073839</td>\n",
       "      <td>1,041,072,504.0000</td>\n",
       "      <td>566938</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...und meinen Bezirk bekommst du als Erbe mit.</td>\n",
       "      <td>2019-05-01 18:28:22.040</td>\n",
       "      <td>2000102330973</td>\n",
       "      <td>2019-05-01 10:28:57.49</td>\n",
       "      <td>1. Mai in Wien: SPÖ fordert von Strache Rücktritt</td>\n",
       "      <td>Inland</td>\n",
       "      <td>Parteien</td>\n",
       "      <td>AlphaRomeo</td>\n",
       "      <td>m</td>\n",
       "      <td>2015-08-28 17:07:41.110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_Posting    ID_Posting_Parent  ID_CommunityIdentity                                     PostingHeadline                                                                                      PostingComment         PostingCreatedAt     ID_Article   ArticlePublishingDate                                       ArticleTitle ArticleChannel ArticleRessortName UserCommunityName UserGender            UserCreatedAt\n",
       "0  1041073586   1,041,073,234.0000                671476  Das hat gestern bereits der Voggenhuber angeführt!  schieder hatte dem inhaltlich nichts entgegenzusetzen. https://www.youtube.com/watch?v=yiJ-sdjn2Zg  2019-05-01 18:21:15.127  2000102330973  2019-05-01 10:28:57.49  1. Mai in Wien: SPÖ fordert von Strache Rücktritt         Inland           Parteien       Ravenspower        NaN  2018-04-14 13:42:28.470\n",
       "1  1041073839   1,041,072,504.0000                566938                                                 NaN                                                      ...und meinen Bezirk bekommst du als Erbe mit.  2019-05-01 18:28:22.040  2000102330973  2019-05-01 10:28:57.49  1. Mai in Wien: SPÖ fordert von Strache Rücktritt         Inland           Parteien        AlphaRomeo          m  2015-08-28 17:07:41.110"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 343160 entries, 0 to 343159\n",
      "Data columns (total 14 columns):\n",
      " #   Column                 Non-Null Count   Dtype  \n",
      "---  ------                 --------------   -----  \n",
      " 0   ID_Posting             343160 non-null  int64  \n",
      " 1   ID_Posting_Parent      237112 non-null  float64\n",
      " 2   ID_CommunityIdentity   343160 non-null  int64  \n",
      " 3   PostingHeadline        93344 non-null   object \n",
      " 4   PostingComment         313870 non-null  object \n",
      " 5   PostingCreatedAt       343160 non-null  object \n",
      " 6   ID_Article             343160 non-null  int64  \n",
      " 7   ArticlePublishingDate  343160 non-null  object \n",
      " 8   ArticleTitle           343160 non-null  object \n",
      " 9   ArticleChannel         343160 non-null  object \n",
      " 10  ArticleRessortName     343160 non-null  object \n",
      " 11  UserCommunityName      343159 non-null  object \n",
      " 12  UserGender             256591 non-null  object \n",
      " 13  UserCreatedAt          343160 non-null  object \n",
      "dtypes: float64(1), int64(3), object(10)\n",
      "memory usage: 36.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_postings_1 = pd.read_csv(file_postings_1, sep=';')\n",
    "display(df_postings_1.head(2))\n",
    "df_postings_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Posting</th>\n",
       "      <th>ID_Posting_Parent</th>\n",
       "      <th>ID_CommunityIdentity</th>\n",
       "      <th>PostingHeadline</th>\n",
       "      <th>PostingComment</th>\n",
       "      <th>PostingCreatedAt</th>\n",
       "      <th>ID_Article</th>\n",
       "      <th>ArticlePublishingDate</th>\n",
       "      <th>ArticleTitle</th>\n",
       "      <th>ArticleChannel</th>\n",
       "      <th>ArticleRessortName</th>\n",
       "      <th>UserCommunityName</th>\n",
       "      <th>UserGender</th>\n",
       "      <th>UserCreatedAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1041515171</td>\n",
       "      <td>NaN</td>\n",
       "      <td>182351</td>\n",
       "      <td>da hat er aber recht ...auch wenn hier nun einige noch immer Naive...</td>\n",
       "      <td>denn Österreich ist von lauter sicheren (EU) Ländern umgeben...die Migranten kommen vorher schon über x sichere Drittländer(=GFK -nur im 1.sicheren Flüchtlingsstatus)... und detto sein Argument zu Griechenland (bei Italien reduziert sich das durch Salvinis Konsequenz gegen sg. \"Retter\" d. kurz vor d. Libyschen Küste wiedermal Schleppern d. Arbeit abnhemen und illegal in d. EU transportieren wollen UND es gab eine EU-Gipfelbschluß im Juni 2018 das auszutrocknen so what? Ö hat nebenbei pro Kopf d. höchsten Zahlen an sg. \"Flüchtlingen\" &amp; auch sogar Anerennungen  (+ was vergessen wird: kumulativ!) - auch hier schon mal geschrieben https://derstandard.at/2000082091102/Was-aus-liberaler-Sicht-fuer-eine-Festung-Europa-spricht Es wird also Zeit</td>\n",
       "      <td>2019-05-16 11:25:39.287</td>\n",
       "      <td>2000103241947</td>\n",
       "      <td>2019-05-16 10:57:22.00</td>\n",
       "      <td>Innenminister Kickl will überhaupt keine Asylanträge mehr</td>\n",
       "      <td>Inland</td>\n",
       "      <td>Integrationspolitik</td>\n",
       "      <td>nadaschauichaber</td>\n",
       "      <td>m</td>\n",
       "      <td>2012-11-25 15:09:03.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1041515292</td>\n",
       "      <td>1,041,514,595.0000</td>\n",
       "      <td>182351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>außer von den Naiven die aus 2015 nichts gelernt haben und am Rechtsruck in Europa damit schuld sind - denn keiner will mehr weitere allzuheftiges enstastand inzwischen schon  https://www.sueddeutsche.de/news/panorama/kriminalitaet---duesseldorf-die-ehre-der-familie-lagebild-sieht-104-kriminelle-clans-dpa.urn-newsml-dpa-com-20090101-190514-99-218262 bzw https://www.deutschlandfunk.de/erstes-lagebild-clankriminalitaet-im-kampf-gegen.720.de.html?dram:article_id=448878  wollen wir das auch?</td>\n",
       "      <td>2019-05-16 11:28:44.703</td>\n",
       "      <td>2000103241947</td>\n",
       "      <td>2019-05-16 10:57:22.00</td>\n",
       "      <td>Innenminister Kickl will überhaupt keine Asylanträge mehr</td>\n",
       "      <td>Inland</td>\n",
       "      <td>Integrationspolitik</td>\n",
       "      <td>nadaschauichaber</td>\n",
       "      <td>m</td>\n",
       "      <td>2012-11-25 15:09:03.087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_Posting    ID_Posting_Parent  ID_CommunityIdentity                                                        PostingHeadline                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              PostingComment         PostingCreatedAt     ID_Article   ArticlePublishingDate                                               ArticleTitle ArticleChannel   ArticleRessortName UserCommunityName UserGender            UserCreatedAt\n",
       "0  1041515171                  NaN                182351  da hat er aber recht ...auch wenn hier nun einige noch immer Naive...  denn Österreich ist von lauter sicheren (EU) Ländern umgeben...die Migranten kommen vorher schon über x sichere Drittländer(=GFK -nur im 1.sicheren Flüchtlingsstatus)... und detto sein Argument zu Griechenland (bei Italien reduziert sich das durch Salvinis Konsequenz gegen sg. \"Retter\" d. kurz vor d. Libyschen Küste wiedermal Schleppern d. Arbeit abnhemen und illegal in d. EU transportieren wollen UND es gab eine EU-Gipfelbschluß im Juni 2018 das auszutrocknen so what? Ö hat nebenbei pro Kopf d. höchsten Zahlen an sg. \"Flüchtlingen\" & auch sogar Anerennungen  (+ was vergessen wird: kumulativ!) - auch hier schon mal geschrieben https://derstandard.at/2000082091102/Was-aus-liberaler-Sicht-fuer-eine-Festung-Europa-spricht Es wird also Zeit  2019-05-16 11:25:39.287  2000103241947  2019-05-16 10:57:22.00  Innenminister Kickl will überhaupt keine Asylanträge mehr         Inland  Integrationspolitik  nadaschauichaber          m  2012-11-25 15:09:03.087\n",
       "1  1041515292   1,041,514,595.0000                182351                                                                    NaN                                                                                                                                                                                                                                                                außer von den Naiven die aus 2015 nichts gelernt haben und am Rechtsruck in Europa damit schuld sind - denn keiner will mehr weitere allzuheftiges enstastand inzwischen schon  https://www.sueddeutsche.de/news/panorama/kriminalitaet---duesseldorf-die-ehre-der-familie-lagebild-sieht-104-kriminelle-clans-dpa.urn-newsml-dpa-com-20090101-190514-99-218262 bzw https://www.deutschlandfunk.de/erstes-lagebild-clankriminalitaet-im-kampf-gegen.720.de.html?dram:article_id=448878  wollen wir das auch?  2019-05-16 11:28:44.703  2000103241947  2019-05-16 10:57:22.00  Innenminister Kickl will überhaupt keine Asylanträge mehr         Inland  Integrationspolitik  nadaschauichaber          m  2012-11-25 15:09:03.087"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 395934 entries, 0 to 395933\n",
      "Data columns (total 14 columns):\n",
      " #   Column                 Non-Null Count   Dtype  \n",
      "---  ------                 --------------   -----  \n",
      " 0   ID_Posting             395934 non-null  int64  \n",
      " 1   ID_Posting_Parent      263201 non-null  float64\n",
      " 2   ID_CommunityIdentity   395934 non-null  int64  \n",
      " 3   PostingHeadline        113953 non-null  object \n",
      " 4   PostingComment         363437 non-null  object \n",
      " 5   PostingCreatedAt       395934 non-null  object \n",
      " 6   ID_Article             395934 non-null  int64  \n",
      " 7   ArticlePublishingDate  395934 non-null  object \n",
      " 8   ArticleTitle           395934 non-null  object \n",
      " 9   ArticleChannel         395934 non-null  object \n",
      " 10  ArticleRessortName     395934 non-null  object \n",
      " 11  UserCommunityName      395934 non-null  object \n",
      " 12  UserGender             293078 non-null  object \n",
      " 13  UserCreatedAt          395934 non-null  object \n",
      "dtypes: float64(1), int64(3), object(10)\n",
      "memory usage: 42.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_postings_2 = pd.read_csv(file_postings_2, sep=';')\n",
    "display(df_postings_2.head(2))\n",
    "df_postings_2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Concat Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 739094 entries, 0 to 739093\n",
      "Data columns (total 14 columns):\n",
      " #   Column                 Non-Null Count   Dtype         \n",
      "---  ------                 --------------   -----         \n",
      " 0   ID_Posting             739094 non-null  int64         \n",
      " 1   ID_Posting_Parent      500313 non-null  float64       \n",
      " 2   ID_CommunityIdentity   739094 non-null  int64         \n",
      " 3   PostingHeadline        207297 non-null  object        \n",
      " 4   PostingComment         677307 non-null  object        \n",
      " 5   PostingCreatedAt       739094 non-null  datetime64[ns]\n",
      " 6   ID_Article             739094 non-null  int64         \n",
      " 7   ArticlePublishingDate  739094 non-null  datetime64[ns]\n",
      " 8   ArticleTitle           739094 non-null  object        \n",
      " 9   ArticleChannel         739094 non-null  object        \n",
      " 10  ArticleRessortName     739094 non-null  object        \n",
      " 11  UserCommunityName      739093 non-null  object        \n",
      " 12  UserGender             549669 non-null  object        \n",
      " 13  UserCreatedAt          739094 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](3), float64(1), int64(3), object(7)\n",
      "memory usage: 78.9+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Posting</th>\n",
       "      <th>ID_Posting_Parent</th>\n",
       "      <th>ID_CommunityIdentity</th>\n",
       "      <th>PostingHeadline</th>\n",
       "      <th>PostingComment</th>\n",
       "      <th>PostingCreatedAt</th>\n",
       "      <th>ID_Article</th>\n",
       "      <th>ArticlePublishingDate</th>\n",
       "      <th>ArticleTitle</th>\n",
       "      <th>ArticleChannel</th>\n",
       "      <th>ArticleRessortName</th>\n",
       "      <th>UserCommunityName</th>\n",
       "      <th>UserGender</th>\n",
       "      <th>UserCreatedAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1041073586</td>\n",
       "      <td>1,041,073,234.0000</td>\n",
       "      <td>671476</td>\n",
       "      <td>Das hat gestern bereits der Voggenhuber angeführt!</td>\n",
       "      <td>schieder hatte dem inhaltlich nichts entgegenzusetzen. https://www.youtube.com/watch?v=yiJ-sdjn2Zg</td>\n",
       "      <td>2019-05-01 18:21:15.127</td>\n",
       "      <td>2000102330973</td>\n",
       "      <td>2019-05-01 10:28:57.490</td>\n",
       "      <td>1. Mai in Wien: SPÖ fordert von Strache Rücktritt</td>\n",
       "      <td>Inland</td>\n",
       "      <td>Parteien</td>\n",
       "      <td>Ravenspower</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-04-14 13:42:28.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1041073839</td>\n",
       "      <td>1,041,072,504.0000</td>\n",
       "      <td>566938</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...und meinen Bezirk bekommst du als Erbe mit.</td>\n",
       "      <td>2019-05-01 18:28:22.040</td>\n",
       "      <td>2000102330973</td>\n",
       "      <td>2019-05-01 10:28:57.490</td>\n",
       "      <td>1. Mai in Wien: SPÖ fordert von Strache Rücktritt</td>\n",
       "      <td>Inland</td>\n",
       "      <td>Parteien</td>\n",
       "      <td>AlphaRomeo</td>\n",
       "      <td>m</td>\n",
       "      <td>2015-08-28 17:07:41.110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_Posting    ID_Posting_Parent  ID_CommunityIdentity                                     PostingHeadline                                                                                      PostingComment        PostingCreatedAt     ID_Article   ArticlePublishingDate                                       ArticleTitle ArticleChannel ArticleRessortName UserCommunityName UserGender           UserCreatedAt\n",
       "0  1041073586   1,041,073,234.0000                671476  Das hat gestern bereits der Voggenhuber angeführt!  schieder hatte dem inhaltlich nichts entgegenzusetzen. https://www.youtube.com/watch?v=yiJ-sdjn2Zg 2019-05-01 18:21:15.127  2000102330973 2019-05-01 10:28:57.490  1. Mai in Wien: SPÖ fordert von Strache Rücktritt         Inland           Parteien       Ravenspower        NaN 2018-04-14 13:42:28.470\n",
       "1  1041073839   1,041,072,504.0000                566938                                                 NaN                                                      ...und meinen Bezirk bekommst du als Erbe mit. 2019-05-01 18:28:22.040  2000102330973 2019-05-01 10:28:57.490  1. Mai in Wien: SPÖ fordert von Strache Rücktritt         Inland           Parteien        AlphaRomeo          m 2015-08-28 17:07:41.110"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat postings1 and postings2 in order! \n",
    "df_postings = pd.concat([df_postings_1, df_postings_2], ignore_index=True)\n",
    "\n",
    "df_postings[['PostingCreatedAt', 'ArticlePublishingDate', 'UserCreatedAt']] = df_postings[['PostingCreatedAt', 'ArticlePublishingDate', 'UserCreatedAt']].astype('datetime64')\n",
    "\n",
    "df_postings.info()\n",
    "df_postings.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(739094, 14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO is it necessary to subset the data because it is too large?\n",
    "\n",
    "#df_postings['PostingCreatedAt'].dt.date.head()\n",
    "# df_postings = df_postings[pd.to_datetime(df_postings['PostingCreatedAt'].dt.date) == '2019-05-01']\n",
    "df_postings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation 1: User_A commented/posted to post of User_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Edge relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of iterating over the dataframe rows to create a edge list, I use a join, which is way faster on the whole dataset. <br>\n",
    "But, by default `merge` changes the sort order, so you have to sort by the original dataframe index!<br>\n",
    "The following code creates a dataframe for posting source user and target posting user. It has the same shape as the original dataframe `df_postings`.\n",
    "So, target posting user may be `NaN`, due to left join and preserving all observations in the posting dataframe.\n",
    "One could filter the Target_User column by not `NaN` to get only relations between users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(739094, 14)\n",
      "(739094, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Posting</th>\n",
       "      <th>PostingCreatedAt</th>\n",
       "      <th>Source_User</th>\n",
       "      <th>Target_User</th>\n",
       "      <th>PostingCreatedAtDay</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1041073586</td>\n",
       "      <td>2019-05-01 18:21:15.127</td>\n",
       "      <td>671476</td>\n",
       "      <td>233,191.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1041073839</td>\n",
       "      <td>2019-05-01 18:28:22.040</td>\n",
       "      <td>566938</td>\n",
       "      <td>640,123.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1041073872</td>\n",
       "      <td>2019-05-01 18:29:05.533</td>\n",
       "      <td>669286</td>\n",
       "      <td>680,772.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1041080734</td>\n",
       "      <td>2019-05-01 22:37:56.010</td>\n",
       "      <td>671476</td>\n",
       "      <td>51,817.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1041080828</td>\n",
       "      <td>2019-05-01 22:42:06.310</td>\n",
       "      <td>671476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID_Posting        PostingCreatedAt  Source_User          Target_User  PostingCreatedAtDay\n",
       "index                                                                                           \n",
       "0      1041073586 2019-05-01 18:21:15.127       671476         233,191.0000                    1\n",
       "1      1041073839 2019-05-01 18:28:22.040       566938         640,123.0000                    1\n",
       "2      1041073872 2019-05-01 18:29:05.533       669286         680,772.0000                    1\n",
       "3      1041080734 2019-05-01 22:37:56.010       671476          51,817.0000                    1\n",
       "4      1041080828 2019-05-01 22:42:06.310       671476                  NaN                    1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 739094 entries, 0 to 739093\n",
      "Data columns (total 5 columns):\n",
      " #   Column               Non-Null Count   Dtype         \n",
      "---  ------               --------------   -----         \n",
      " 0   ID_Posting           739094 non-null  int64         \n",
      " 1   PostingCreatedAt     739094 non-null  datetime64[ns]\n",
      " 2   Source_User          739094 non-null  int64         \n",
      " 3   Target_User          500312 non-null  float64       \n",
      " 4   PostingCreatedAtDay  739094 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(1), int64(3)\n",
      "memory usage: 33.8 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "postings_user_commented_relation_df = df_postings.reset_index()[['index', 'ID_Posting', 'ID_CommunityIdentity', 'PostingCreatedAt', 'ID_Posting_Parent']].merge(\n",
    "    df_postings[['ID_Posting', 'ID_CommunityIdentity']], \n",
    "    left_on='ID_Posting_Parent', right_on='ID_Posting',\n",
    "    suffixes=('', '_parent'), how='left', sort=False\n",
    ")[['index', 'ID_Posting', 'PostingCreatedAt', 'ID_CommunityIdentity', 'ID_CommunityIdentity_parent']].sort_values(by='index')\n",
    "\n",
    "postings_user_commented_relation_df.set_index('index', inplace=True) # index may be useful for selection?\n",
    "# postings_user_commented_relation_df.set_index(['ID_Posting', 'PostingCreatedAt'], inplace=True) # index may be useful for selection?\n",
    "postings_user_commented_relation_df.rename(columns={'ID_CommunityIdentity': 'Source_User', 'ID_CommunityIdentity_parent': 'Target_User'}, inplace=True)\n",
    "\n",
    "# postings_user_commented_relation_df.drop('index', axis='columns', inplace=True) # index is ascending as original df_postings, index column not needed anymore\n",
    "\n",
    "# SAME number of rows and SAME order as original df_postings:\n",
    "print(df_postings.shape)\n",
    "print(postings_user_commented_relation_df.shape)\n",
    "assert df_postings.shape[0] == postings_user_commented_relation_df.shape[0]\n",
    "\n",
    "# add column PostingCreatedAtDay to have easy access to Day of month\n",
    "postings_user_commented_relation_df['PostingCreatedAtDay'] = postings_user_commented_relation_df['PostingCreatedAt'].dt.day\n",
    "\n",
    "display(postings_user_commented_relation_df.head(5))\n",
    "display(postings_user_commented_relation_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_created_at(series, day_start, day_end):\n",
    "    \"\"\"\n",
    "    create a boolean mask of series where day is in day_start incl. and day_end_incl.\n",
    "    \"\"\"\n",
    "    return series.isin(range(day_start, day_end+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Posting</th>\n",
       "      <th>PostingCreatedAt</th>\n",
       "      <th>Source_User</th>\n",
       "      <th>Target_User</th>\n",
       "      <th>PostingCreatedAtDay</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>261921</th>\n",
       "      <td>1041122420</td>\n",
       "      <td>2019-05-03 13:15:27.730</td>\n",
       "      <td>652259</td>\n",
       "      <td>572,922.0000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274585</th>\n",
       "      <td>1041124660</td>\n",
       "      <td>2019-05-03 14:23:14.000</td>\n",
       "      <td>634552</td>\n",
       "      <td>500,307.0000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214053</th>\n",
       "      <td>1041136645</td>\n",
       "      <td>2019-05-03 21:31:47.440</td>\n",
       "      <td>584635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40877</th>\n",
       "      <td>1041106463</td>\n",
       "      <td>2019-05-02 20:57:40.253</td>\n",
       "      <td>690687</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63312</th>\n",
       "      <td>1041132144</td>\n",
       "      <td>2019-05-03 18:34:18.970</td>\n",
       "      <td>35950</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289961</th>\n",
       "      <td>1041124479</td>\n",
       "      <td>2019-05-03 14:16:08.487</td>\n",
       "      <td>666841</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213338</th>\n",
       "      <td>1041121051</td>\n",
       "      <td>2019-05-03 12:37:15.593</td>\n",
       "      <td>173358</td>\n",
       "      <td>222,059.0000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12658</th>\n",
       "      <td>1041127145</td>\n",
       "      <td>2019-05-03 15:42:57.790</td>\n",
       "      <td>548063</td>\n",
       "      <td>635,120.0000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23537</th>\n",
       "      <td>1041105562</td>\n",
       "      <td>2019-05-02 20:23:52.823</td>\n",
       "      <td>499569</td>\n",
       "      <td>2,968.0000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50516</th>\n",
       "      <td>1041100737</td>\n",
       "      <td>2019-05-02 17:04:43.847</td>\n",
       "      <td>558714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID_Posting        PostingCreatedAt  Source_User          Target_User  PostingCreatedAtDay\n",
       "index                                                                                            \n",
       "261921  1041122420 2019-05-03 13:15:27.730       652259         572,922.0000                    3\n",
       "274585  1041124660 2019-05-03 14:23:14.000       634552         500,307.0000                    3\n",
       "214053  1041136645 2019-05-03 21:31:47.440       584635                  NaN                    3\n",
       "40877   1041106463 2019-05-02 20:57:40.253       690687                  NaN                    2\n",
       "63312   1041132144 2019-05-03 18:34:18.970        35950                  NaN                    3\n",
       "289961  1041124479 2019-05-03 14:16:08.487       666841                  NaN                    3\n",
       "213338  1041121051 2019-05-03 12:37:15.593       173358         222,059.0000                    3\n",
       "12658   1041127145 2019-05-03 15:42:57.790       548063         635,120.0000                    3\n",
       "23537   1041105562 2019-05-02 20:23:52.823       499569           2,968.0000                    2\n",
       "50516   1041100737 2019-05-02 17:04:43.847       558714                  NaN                    2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[3, 2, 4]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_posting_created_at = get_mask_created_at(postings_user_commented_relation_df['PostingCreatedAtDay'], \n",
    "                                                    day_start=2, day_end=4)\n",
    "\n",
    "display(postings_user_commented_relation_df.loc[mask_posting_created_at].sample(10))\n",
    "list(postings_user_commented_relation_df.loc[mask_posting_created_at]['PostingCreatedAtDay'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful when using the df_posting dataframe without `NAN` values below. Some operations require the original `df_postings` order (NLP text extraction)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(739094, 5)\n",
      "(500312, 5)\n"
     ]
    }
   ],
   "source": [
    "postings_user_commented_relation_nonan_df = postings_user_commented_relation_df.dropna()\n",
    "# check number of rows after dropping NAN\n",
    "print(postings_user_commented_relation_df.shape)\n",
    "print(postings_user_commented_relation_nonan_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute graph topology features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function can compute graph features based on posting relation data frame and start and end day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computation_input_df_combined_postings(postings_relation_df, day_start, day_end):\n",
    "    \"\"\"\n",
    "    day_start incl. and day_end incl.\n",
    "    \"\"\"\n",
    "    print('computation_input_df_combined_postings: Days {}-{}'.format(str(day_start),str(day_end)))\n",
    "    assert (day_start <= day_end) & (day_start > 0) & (day_end <= 31)\n",
    "\n",
    "    postings_relation_df = postings_relation_df.copy()\n",
    "\n",
    "    # get postings relations where day is between day_start and day_end\n",
    "    mask_posting_created_at = get_mask_created_at(postings_relation_df['PostingCreatedAtDay'], \n",
    "                                                  day_start=day_start, day_end=day_end)\n",
    "    postings_relation_df_day_subset = postings_relation_df.loc[mask_posting_created_at]\n",
    "    # assert all postings are created between day_start and day_end:\n",
    "    assert postings_relation_df_day_subset['PostingCreatedAtDay'].isin(range(day_start, day_end+1)).all()\n",
    "\n",
    "    # display(postings_relation_df_day_subset.sample(4))\n",
    "\n",
    "    # drop NaN in Target_User, because these rows have no relation:\n",
    "    postings_relation_df_day_subset_no_nan = postings_relation_df_day_subset.dropna().copy()\n",
    "    postings_relation_df_day_subset_no_nan['Target_User'] = postings_relation_df_day_subset_no_nan['Target_User'].astype('int64')\n",
    "\n",
    "    #display(postings_relation_df_day_subset_no_nan.sample(4))\n",
    "    #display(postings_relation_df_day_subset_no_nan.info())\n",
    "\n",
    "    # obtain interaction count as edge weight\n",
    "    postings_relation_df_day_subset_no_nan_count = postings_relation_df_day_subset_no_nan[['ID_Posting', 'Source_User', 'Target_User']].groupby(\n",
    "                                                        ['Source_User', 'Target_User'])['ID_Posting'].count().reset_index(name=\"weight\")\n",
    "\n",
    "    #display(postings_relation_df_day_subset_no_nan_count.sample(4))\n",
    "    #display(postings_relation_df_day_subset_no_nan_count.info())\n",
    "\n",
    "    #Creation of directed graph for day range\n",
    "    G_postings_commented_days_sub_directed = nx.from_pandas_edgelist(postings_relation_df_day_subset_no_nan_count,\n",
    "                                                                     source='Source_User',\n",
    "                                                                     target='Target_User',\n",
    "                                                                     edge_attr='weight',\n",
    "                                                                     create_using=nx.DiGraph())\n",
    "    print('Created a', nx.info(G_postings_commented_days_sub_directed))\n",
    "    # assert that the graph has weight as edge attribute with a weight > 0 (should be weight column in df)\n",
    "    assert G_postings_commented_days_sub_directed.get_edge_data(*tuple(postings_relation_df_day_subset_no_nan_count.loc[0, ['Source_User', 'Target_User']]))['weight'] > 0\n",
    "\n",
    "    G_postings_commented_days_sub_undirected = nx.from_pandas_edgelist(postings_relation_df_day_subset_no_nan_count,\n",
    "                                                                       source='Source_User',\n",
    "                                                                       target='Target_User',\n",
    "                                                                       create_using=nx.Graph())\n",
    "    print('Created a', nx.info(G_postings_commented_days_sub_undirected))\n",
    "\n",
    "    # node list, ebunch is pairs of node and should be of length (len(nodes)*(len(nodes)-1)):\n",
    "    nodes = list(G_postings_commented_days_sub_directed.nodes)\n",
    "    ebunch = [(i,j) for i,j in product(nodes, nodes) if i!=j] # all node pairs\n",
    "    assert len(ebunch) == (len(nodes)*(len(nodes)-1))\n",
    "\n",
    "    print(f'{datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} Starting computation')\n",
    "\n",
    "    # Computation of Jaccard coefficient\n",
    "    jac = nx.jaccard_coefficient(G_postings_commented_days_sub_undirected, ebunch)\n",
    "    jac_df = pd.DataFrame(list(jac), columns = ['Source_User', 'Target_User', 'jaccard_coef'])\n",
    "    assert jac_df.shape[0] == (len(nodes)*(len(nodes)-1))\n",
    "    print(f'{datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} Jaccard: ok!')\n",
    "\n",
    "    # Computation of Adamic and Adar index\n",
    "    ad_ad = nx.adamic_adar_index(G_postings_commented_days_sub_undirected, ebunch)\n",
    "    ad_ad_df = pd.DataFrame(list(ad_ad), columns = ['Source_User', 'Target_User', 'adamic_adar_index'])\n",
    "    assert ad_ad_df.shape[0] == (len(nodes)*(len(nodes)-1))\n",
    "    print(f'{datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} Adamic and Adar: ok!')\n",
    "    result_df = jac_df.merge(ad_ad_df)\n",
    "\n",
    "    # Computation of Preferential Attachment\n",
    "    pref_att = nx.preferential_attachment(G_postings_commented_days_sub_undirected, ebunch)\n",
    "    pref_att_df = pd.DataFrame(list(pref_att), columns = ['Source_User', 'Target_User', 'preferential_attachment_index'])\n",
    "    assert pref_att_df.shape[0] == (len(nodes)*(len(nodes)-1))\n",
    "    print(f'{datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} Preferential Attachment: ok!')\n",
    "    result_df = result_df.merge(pref_att_df)\n",
    "\n",
    "    # Clustering coefficient score (on directed graph)\n",
    "    cluster_dict = nx.clustering(G_postings_commented_days_sub_directed)\n",
    "    cluster_dict_df = pd.DataFrame.from_dict(cluster_dict, orient='index', columns=['clustering_coefficient_score']).reset_index().rename(columns={'index':'User'})\n",
    "    assert cluster_dict_df.shape[0] == len(nodes)\n",
    "    #display(cluster_dict_df.head())\n",
    "    # Add a column for clustering_coefficient_score_Source_User using the Clustering coefficient for the Source_User node\n",
    "    result_df = result_df.merge(cluster_dict_df.rename(columns={'User': 'Source_User', \n",
    "                                                               'clustering_coefficient_score': 'clustering_coefficient_score_Source_User'}),\n",
    "                                on='Source_User', how='left')\n",
    "    # Add a column for clustering_coefficient_score_Target_User using the Clustering coefficient for the Target_User node\n",
    "    result_df = result_df.merge(cluster_dict_df.rename(columns={'User': 'Target_User', \n",
    "                                                               'clustering_coefficient_score': 'clustering_coefficient_score_Target_User'}),\n",
    "                                on='Target_User', how='left')\n",
    "    print(f'{datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} Clustering coefficient: ok!')\n",
    "    \n",
    "    # Pagerank (on directed graph)\n",
    "    pagerank_dict = nx.pagerank(G_postings_commented_days_sub_directed)\n",
    "    pagerank_dict_df = pd.DataFrame.from_dict(pagerank_dict, orient='index', columns=['pagerank']).reset_index().rename(columns={'index':'User'})\n",
    "    assert pagerank_dict_df.shape[0] == len(nodes)\n",
    "     # Add a column for pagerank_Source_User using the Pagerank for the Source_User node\n",
    "    result_df = result_df.merge(pagerank_dict_df.rename(columns={'User': 'Source_User', \n",
    "                                                               'pagerank': 'pagerank_Source_User'}),\n",
    "                                on='Source_User', how='left')\n",
    "    # Add a column for pagerank_Target_User using the Pagerank for the Target_User node\n",
    "    result_df = result_df.merge(pagerank_dict_df.rename(columns={'User': 'Target_User', \n",
    "                                                               'pagerank': 'pagerank_Target_User'}),\n",
    "                                on='Target_User', how='left')\n",
    "    print(f'{datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} Pagerank: ok!')\n",
    "    print('Resulting result_df has shape', result_df.shape)\n",
    "    assert result_df.shape[0] == (len(nodes)*(len(nodes)-1))\n",
    "    display(result_df.head(5))\n",
    "    #display(result_df[result_df['clustering_coefficient_score_Target_User'] > 0])\n",
    "\n",
    "    # Write resulting graph topology features for Source_User and Target_User combinations to a file\n",
    "    pickle_out_file_path_features = 'output/G_features_postings_days_{}-{}.pkl'.format(str(day_start), str(day_end))\n",
    "    with open(pickle_out_file_path_features, 'wb') as f:\n",
    "        pickle.dump(result_df, f)\n",
    "    print(f'{datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} Successfully written result_df to file \"{pickle_out_file_path_features}\".')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute graph features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate for days and save to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_graph_features = computation_input_df_combined_postings(postings_user_commented_relation_df, day_start=1, day_end=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_graph_features = computation_input_df_combined_postings(postings_user_commented_relation_df, day_start=1, day_end=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_graph_features = computation_input_df_combined_postings(postings_user_commented_relation_df, day_start=2, day_end=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_graph_features = computation_input_df_combined_postings(postings_user_commented_relation_df, day_start=3, day_end=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_graph_features = computation_input_df_combined_postings(postings_user_commented_relation_df, day_start=4, day_end=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_graph_features = computation_input_df_combined_postings(postings_user_commented_relation_df, day_start=6, day_end=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_graph_features = computation_input_df_combined_postings(postings_user_commented_relation_df, day_start=6, day_end=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_graph_features = computation_input_df_combined_postings(postings_user_commented_relation_df, day_start=7, day_end=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_graph_features = computation_input_df_combined_postings(postings_user_commented_relation_df, day_start=8, day_end=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_graph_features = computation_input_df_combined_postings(postings_user_commented_relation_df, day_start=9, day_end=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with some NLP extractions of text and one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download spacy german language model - used for tokenization, lemmatization of article text (vector data)\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot encoding (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot encoding of `'ArticleChannel'`, `'ArticleRessortName'` for ML pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# encode article channel and article ressort name with one hot encoding\n",
    "\n",
    "# TODO use it like that or use the TEST set for one hot encoding. \n",
    "# Assuming all article channels and article resort names are known a priori this is ok imho for derStandard.\n",
    "# Just use a subset of df_posting e.g. certain days, if the number of features is too high in ML pipeline...\n",
    "\n",
    "\n",
    "# TODO be aware of the order when adding it\n",
    "\n",
    "# ArticleChannel\tArticleRessortName\n",
    "one_hot_encoder_article_channel = OneHotEncoder(handle_unknown='ignore').fit(df_postings[['ArticleChannel', 'ArticleRessortName']])\n",
    "\n",
    "print(one_hot_encoder_article_channel.categories_)\n",
    "\n",
    "transformed_channel_resort = one_hot_encoder_article_channel.transform(df_postings[['ArticleChannel', 'ArticleRessortName']])\n",
    "print(transformed_channel_resort.shape)\n",
    "display(transformed_channel_resort.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF feature extraction (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF feature extraction of combined `PostingHeadline`, `PostingComment` and `ArticleTitle` next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_spacy = 'output/combinedPostingText.spacy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step for creating `'output/combinedPostingText.spacy'` is usually not necessary. It takes around 64 minutes and you can just use my file: <br>\n",
    "Run the next chunk, if you want to wait long for computing the `\"combinedPostingText.spacy\"` file.<br>\n",
    "`doc_bin` holds document information in order of `df_postings`. Existing `combinedPostingText.spacy` stores nlp information for all `df_postings` rows, not just a subset of days. Take care of the order of `df_postings`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "df_postings['CombinedArticlePostingText'] = df_postings['PostingHeadline'].fillna('') + ' ' + df_postings['PostingComment'].fillna('') + ' ' + df_postings['ArticleTitle'].fillna('')\n",
    "\n",
    "doc_bin = DocBin(attrs=[\"LEMMA\", \"ENT_IOB\", \"ENT_TYPE\"], store_user_data=True)\n",
    "\n",
    "for doc in nlp.pipe(df_postings['CombinedArticlePostingText'].str.lower()):\n",
    "    #print(repr(doc))\n",
    "    doc_bin.add(doc)\n",
    "\n",
    "#bytes_data = doc_bin.to_bytes()\n",
    "#doc_bin = DocBin().from_bytes(bytes_data)\n",
    "\n",
    "doc_bin.to_disk(file_name_spacy)\n",
    "\n",
    "# save tokenized spacy Doc objects to file\n",
    "# the file contains the combined text of posting headline, posting comment and article title in a tokenized form\n",
    "# and also meta data about a token, like its lemma form, whether a token is a punctuation, or bracket etc.\n",
    "# processing took around 64 minutes on the whole df_postings for me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next takes around 5 minutes for me. You could also just use the pickle file `tokenized_lemmatized_texts.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "doc_bin = DocBin().from_disk(file_name_spacy)\n",
    "\n",
    "docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "#print(docs)\n",
    "print(len(docs))\n",
    "tokenized_lemmatized_texts = [[token.lemma_ for token in doc \n",
    "                               if not token.is_stop and not token.is_punct and not token.is_space \n",
    "                               and not token.like_url and not token.like_email and not token.is_quote \n",
    "                               and not token.is_currency and not token.is_bracket and not token.is_quote\n",
    "                               and not token.is_left_punct and not token.is_right_punct and not token.is_bracket]\n",
    "                               for doc in docs]\n",
    "print(len(tokenized_lemmatized_texts))\n",
    "print(tokenized_lemmatized_texts[0])\n",
    "print(tokenized_lemmatized_texts[100])\n",
    "\n",
    "# tokenized_lemmatized_texts contains nlp information for all documents in df_postings in order!\n",
    "\n",
    "# save as pickle file:\n",
    "with open('output/tokenized_lemmatized_texts.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenized_lemmatized_texts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue here: Load the pickled file `tokenized_lemmatized_texts.pkl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/tokenized_lemmatized_texts.pkl', 'rb') as f:\n",
    "    tokenized_lemmatized_texts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_lemmatized_texts contains nlp information for all documents in df_postings in order!\n",
    "# is a list of list of tokens, a list of tokens for the combined texts for a posting-article-combination for a certain posting entry in posting_df.\n",
    "print(tokenized_lemmatized_texts[0:2])\n",
    "print(len(tokenized_lemmatized_texts))\n",
    "print(len(df_postings))\n",
    "\n",
    "assert len(tokenized_lemmatized_texts) == len(df_postings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show an example of utilizing the NLP tf-idf features by adding them to the matrix of graph features.<br>\n",
    "The example is illustrated with days 1–4 combined. We may use these graph feature for Training and Testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set - graph based features used are from days 1--4 combined.\n",
    "# training set - vetor-based NLP features used are from days 1--4:\n",
    "\n",
    "# filter tokenized_lemmatized_texts by days of PostingCreatedAt:\n",
    "mask_posting_created_at_days_1to4 = get_mask_created_at(postings_user_commented_relation_df['PostingCreatedAtDay'],\n",
    "                                                        day_start=1, day_end=4)\n",
    "# filter target user is not nan\n",
    "mask_target_user_non_nan = ~postings_user_commented_relation_df['Target_User'].isna()\n",
    "\n",
    "mask_posting_days_1to4_target_user_non_nan = (mask_posting_created_at_days_1to4 & mask_target_user_non_nan)\n",
    "\n",
    "tokenized_lemmatized_texts_at_days_1to4 = list(compress(tokenized_lemmatized_texts, list(mask_posting_days_1to4_target_user_non_nan)))\n",
    "assert (len(tokenized_lemmatized_texts_at_days_1to4)) == mask_posting_days_1to4_target_user_non_nan.sum()\n",
    "display(tokenized_lemmatized_texts_at_days_1to4[0:1])\n",
    "print('There are', len(tokenized_lemmatized_texts_at_days_1to4),'tokenized-lemmatized texts for these days where Target_User is not NaN')\n",
    "\n",
    "assert postings_user_commented_relation_df.loc[mask_posting_days_1to4_target_user_non_nan].isna().sum(axis=0).sum() == 0\n",
    "assert postings_user_commented_relation_df.loc[mask_posting_days_1to4_target_user_non_nan]['PostingCreatedAtDay'].isin(range(1, 4+1)).all()\n",
    "assert ~postings_user_commented_relation_df.loc[mask_posting_days_1to4_target_user_non_nan]['PostingCreatedAtDay'].isin(range(5, 31+1)).all()\n",
    "assert len(tokenized_lemmatized_texts_at_days_1to4) == postings_user_commented_relation_df.loc[mask_posting_days_1to4_target_user_non_nan].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tf_idf_features = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO train test split!\n",
    "# only fit on train data and not on test data!\n",
    "\n",
    "# fit tokenized-lemmatized text for days subset on train data:\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), lowercase=False, tokenizer=lambda x: x, max_features=max_tf_idf_features)\n",
    "vectorizer = tf_idf_vectorizer.fit(tokenized_lemmatized_texts_at_days_1to4) # training data\n",
    "print(list(tf_idf_vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain tf-idf vectorized text features for training set:\n",
    "\n",
    "text_tf_idf_features_train = tf_idf_vectorizer.transform(tokenized_lemmatized_texts_at_days_1to4).toarray() # training or test data (here training)\n",
    "print('TF-IDF vectorized text features have shape:', text_tf_idf_features_train.shape)\n",
    "\n",
    "assert len(tokenized_lemmatized_texts_at_days_1to4) == text_tf_idf_features_train.shape[0]\n",
    "assert max_tf_idf_features == text_tf_idf_features_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append tf-idf features to the right of postings_user_commented_relation_df content\n",
    "# BE CAREFUL OF THE ORDER!\n",
    "\n",
    "display(postings_user_commented_relation_df.loc[mask_posting_days_1to4_target_user_non_nan].reset_index(drop=True).head())\n",
    "print(postings_user_commented_relation_df.loc[mask_posting_days_1to4_target_user_non_nan].reset_index(drop=True).shape)\n",
    "\n",
    "display(pd.DataFrame(text_tf_idf_features_train).head())\n",
    "print(pd.DataFrame(text_tf_idf_features_train).shape)\n",
    "\n",
    "# append features\n",
    "postings_user_commented_relation_df_with_tfidf_features = pd.concat([\n",
    "    postings_user_commented_relation_df.loc[mask_posting_days_1to4_target_user_non_nan].reset_index(drop=True), \n",
    "    pd.DataFrame(text_tf_idf_features_train)], axis=1, sort=False)\n",
    "\n",
    "postings_user_commented_relation_df_with_tfidf_features['Target_User'] = postings_user_commented_relation_df_with_tfidf_features['Target_User'].astype('int64')\n",
    "\n",
    "display(postings_user_commented_relation_df_with_tfidf_features)\n",
    "print(postings_user_commented_relation_df_with_tfidf_features.shape)\n",
    "\n",
    "assert postings_user_commented_relation_df_with_tfidf_features.shape[0] == text_tf_idf_features_train.shape[0]\n",
    "assert postings_user_commented_relation_df_with_tfidf_features.shape[1] == (text_tf_idf_features_train.shape[1] + postings_user_commented_relation_df.loc[mask_posting_days_1to4_target_user_non_nan].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to copy the pickle file from our shared Google Drive folder\n",
    "with open('output/G_features_postings_days_1-4.pkl', 'rb') as f:\n",
    "    G_features_postings_days_1to4 = pickle.load(f)\n",
    "# dataframe of graph features for Source_User and Target_User\n",
    "G_features_postings_days_1to4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_features_postings_days_1to4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_tfidf_and_graph_features_only_positive = pd.merge(postings_user_commented_relation_df_with_tfidf_features, G_features_postings_days_1to4,\n",
    "                                                           on=['Source_User', 'Target_User'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_user_commented_relation_df_with_tfidf_features[(postings_user_commented_relation_df_with_tfidf_features['Source_User'] == 671476) & ((postings_user_commented_relation_df_with_tfidf_features['Target_User'] == 233191))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_features_postings_days_1to4[(G_features_postings_days_1to4['Source_User'] == 233191)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(postings_tfidf_and_graph_features_only_positive.shape)\n",
    "postings_tfidf_and_graph_features_only_positive.head()\n",
    "postings_tfidf_and_graph_features_only_positive.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_postings.loc[mask_posting_created_at_days_1to4].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter tokenized_lemmatized_texts by days of PostingCreatedAt:\n",
    "\n",
    "\n",
    "mask_posting_created_at_days_1to4 = get_mask_created_at(postings_user_commented_relation_df['PostingCreatedAtDay'],\n",
    "                                                          day_start=1, day_end=5)\n",
    "tokenized_lemmatized_texts_at_days_1_to_5 = list(compress(tokenized_lemmatized_texts, list(mask_posting_created_at_days_1to4)))\n",
    "\n",
    "assert (len(tokenized_lemmatized_texts_at_days_1_to_5)) == mask_posting_created_at_days_1to4.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO train test split!\n",
    "# only fit on train data and not on test data!\n",
    "# \n",
    "max_tf_idf_features = 300\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), lowercase=False, tokenizer=lambda x: x, max_features=max_tf_idf_features)\n",
    "vectorizer = vectorizer.fit(tokenized_lemmatized_texts)\n",
    "#print(list(vectorizer.get_feature_names_out()))\n",
    "\n",
    "# text_vectorized = vectorizer.transform(tokenized_lemmatized_texts)\n",
    "#print(text_vectorized.toarray().shape)\n",
    "#display(text_vectorized.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a check to see which tf-idf features were extracted:\n",
    "with open('output/feature_names.txt', 'w') as f:\n",
    "    for item in vectorizer.get_feature_names_out():\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to copy the pickle file from our shared Google Drive folder\n",
    "with open('output/G_features_postings_days_1-1.pkl', 'rb') as f:\n",
    "    G_features_postings_days_1 = pickle.load(f)\n",
    "# dataframe of graph features for Source_User and Target_User\n",
    "G_features_postings_days_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_features_postings_days_1[(G_features_postings_days_1['Source_User'] == 671476) & ((G_features_postings_days_1['Target_User'] == 233191))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
