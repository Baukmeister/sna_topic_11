{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Network Analysis - Project sna_topic_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option(\"^display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:20,.4f}'.format)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "The data set is provided by Der Standard, one of the top Austrian newspapers.\n",
    "In the online Standard people can post comments below articles and up/down vote comments.\n",
    "\n",
    "1. There are two files containing all **postings** to articles in May 2019 (due to high amount of data, the postings were split into two files). The respective file also contains additionally meta-data of the postings and articles and some details about the users who composed the postings.\n",
    "\n",
    "2. There are two files containing all **votes** for the postings in point 1 (due to high amount of data, the votes were split into two files). The respective file also contains information whether the vote was negative or positive and some details about the user who did the voting.\n",
    "\n",
    "3. There is one file containing **following and ignoring relationships** among all the users who posted (see point 1) or voted (see point 2) to articles published in May 2019. A following relationship (i.e., the user with the `ID_CommunityIdentity` given in column 1 follows the user with the `ID_CommunityIdentityConnectedTo` given in column 2) is indicated by a “1” in column the `“ID_CommunityConnectionType”`, a ignoring relationship by a “2” in that column (i.e., the user with the `ID_CommunityIdentity` given in column 1 ignores the user with the `ID_CommunityIdentityConnectedTo` given in column 2).\n",
    "\n",
    "There are different entities in the data set: \n",
    "* **Users** - identified by *ID_CommunityIdentity* (or *UserCommunityName*)\n",
    "* **Postings** - identified by *ID_Posting*\n",
    "* **Articles** - identified by *ID_Article*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to datasets:\n",
    "file_postings_1 = 'data/Postings_01052019_15052019.csv'\n",
    "file_postings_2 = 'data/Postings_16052019_31052019.csv'\n",
    "file_votes_1 = 'data/Votes_01052019_15052019.csv'\n",
    "file_votes_2 = 'data/Votes_16052019_31052019.csv'\n",
    "file_following_ignoring = 'data/Following_Ignoring_Relationships_01052019_31052019.csv'\n",
    "\n",
    "output_dir = 'output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_postings_1 = pd.read_csv(file_postings_1, sep=';')\n",
    "display(df_postings_1.head(2))\n",
    "df_postings_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_postings_2 = pd.read_csv(file_postings_2, sep=';')\n",
    "display(df_postings_2.head(2))\n",
    "df_postings_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_votes_1 = pd.read_csv(file_votes_1, sep=';')\n",
    "display(df_votes_1.head(2))\n",
    "df_votes_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_votes_2 = pd.read_csv(file_votes_2, sep=';')\n",
    "display(df_votes_2.head(2))\n",
    "df_votes_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_following_ignoring = pd.read_csv(file_following_ignoring, sep=';')\n",
    "display(df_following_ignoring.head(2))\n",
    "df_following_ignoring.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation 1: User_A commented/posted to post of User_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_postings = pd.concat([df_postings_1, df_postings_2], ignore_index=True)\n",
    "\n",
    "df_postings[['PostingCreatedAt', 'ArticlePublishingDate', 'UserCreatedAt']] = df_postings[['PostingCreatedAt', 'ArticlePublishingDate', 'UserCreatedAt']].astype('datetime64')\n",
    "\n",
    "df_postings.info()\n",
    "df_postings.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting dataset because it is too large\n",
    "\n",
    "#df_postings['PostingCreatedAt'].dt.date.head()\n",
    "df_postings = df_postings[pd.to_datetime(df_postings['PostingCreatedAt'].dt.date) == '2019-05-01']\n",
    "df_postings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with some NLP extractions of text and one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# encode article channel and article ressort name with one hot encoding\n",
    " \n",
    "# ArticleChannel\tArticleRessortName\n",
    "one_hot_encoder_article_channel = OneHotEncoder(handle_unknown='ignore').fit(df_postings[['ArticleChannel', 'ArticleRessortName']])\n",
    "\n",
    "print(one_hot_encoder_article_channel.categories_)\n",
    "\n",
    "transformed_channel_resort = one_hot_encoder_article_channel.transform(df_postings[['ArticleChannel', 'ArticleRessortName']])\n",
    "print(transformed_channel_resort.shape)\n",
    "display(transformed_channel_resort.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO decide on a vectorizer to use:\n",
    "# vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=10000, lowercase=False)\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "def tokenize_lemmatize(doc):\n",
    "    \"\"\"\n",
    "    tokenize, remove stopwords, punctuation and spaces, apply lemmatization to get base form\n",
    "    \"\"\"\n",
    "    #print(doc)\n",
    "    #print(repr([token.lemma_ for token in nlp(doc) if not token.is_stop and not token.is_punct and not token.is_space]))\n",
    "    return [token.lemma_ for token in nlp(doc) if not token.is_stop and not token.is_punct and not token.is_space]\n",
    "\n",
    "def join_text_parts(ph, pc, at):\n",
    "    \"\"\"\n",
    "    combine the three arguments with a space if not empty string\n",
    "    \"\"\"\n",
    "    return ' '.join(filter(None, [ph, pc, at]))\n",
    "\n",
    "text_columns = ['PostingHeadline', 'PostingComment', 'ArticleTitle']\n",
    "#df_postings['CombinedArticlePostingText'] = [join_text_parts(ph,pc,at) for (ph,pc,at) in \n",
    "#                                             df_postings[['PostingHeadline', 'PostingComment', 'ArticleTitle']].fillna('').to_numpy()]\n",
    "\n",
    "df_postings['CombinedArticlePostingText'] = df_postings['PostingHeadline'].fillna('') + ' ' + df_postings['PostingComment'].fillna('') + ' ' + df_postings['ArticleTitle'].fillna('')\n",
    "\n",
    "display(df_postings[['CombinedArticlePostingText'] + text_columns].head(4))\n",
    "\n",
    "# TODO which vectorizer to use?\n",
    "#vectorizer = CountVectorizer(ngram_range=(1, 1), max_features=3000, \n",
    "                            #lowercase=True, tokenizer=tokenize_lemmatize).fit(df_postings['CombinedArticlePostingText'])\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), max_features=3000, lowercase=True, tokenizer=tokenize_lemmatize).fit(df_postings['CombinedArticlePostingText'])\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "res = vectorizer.transform(df_postings['CombinedArticlePostingText'])\n",
    "print(res.shape)\n",
    "\n",
    "# TODO train test split!\n",
    "display(res.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
