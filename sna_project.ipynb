{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Network Analysis - Project sna_topic_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option(\"^display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:20,.4f}'.format)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "The data set is provided by Der Standard, one of the top Austrian newspapers.\n",
    "In the online Standard people can post comments below articles and up/down vote comments.\n",
    "\n",
    "1. There are two files containing all **postings** to articles in May 2019 (due to high amount of data, the postings were split into two files). The respective file also contains additionally meta-data of the postings and articles and some details about the users who composed the postings.\n",
    "\n",
    "2. There are two files containing all **votes** for the postings in point 1 (due to high amount of data, the votes were split into two files). The respective file also contains information whether the vote was negative or positive and some details about the user who did the voting.\n",
    "\n",
    "3. There is one file containing **following and ignoring relationships** among all the users who posted (see point 1) or voted (see point 2) to articles published in May 2019. A following relationship (i.e., the user with the `ID_CommunityIdentity` given in column 1 follows the user with the `ID_CommunityIdentityConnectedTo` given in column 2) is indicated by a “1” in column the `“ID_CommunityConnectionType”`, a ignoring relationship by a “2” in that column (i.e., the user with the `ID_CommunityIdentity` given in column 1 ignores the user with the `ID_CommunityIdentityConnectedTo` given in column 2).\n",
    "\n",
    "There are different entities in the data set: \n",
    "* **Users** - identified by *ID_CommunityIdentity* (or *UserCommunityName*)\n",
    "* **Postings** - identified by *ID_Posting*\n",
    "* **Articles** - identified by *ID_Article*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to datasets:\n",
    "file_postings_1 = 'data/Postings_01052019_15052019.csv'\n",
    "file_postings_2 = 'data/Postings_16052019_31052019.csv'\n",
    "file_votes_1 = 'data/Votes_01052019_15052019.csv'\n",
    "file_votes_2 = 'data/Votes_16052019_31052019.csv'\n",
    "file_following_ignoring = 'data/Following_Ignoring_Relationships_01052019_31052019.csv'\n",
    "\n",
    "output_dir = 'output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_postings_1 = pd.read_csv(file_postings_1, sep=';')\n",
    "display(df_postings_1.head(2))\n",
    "df_postings_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_postings_2 = pd.read_csv(file_postings_2, sep=';')\n",
    "display(df_postings_2.head(2))\n",
    "df_postings_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_votes_1 = pd.read_csv(file_votes_1, sep=';')\n",
    "display(df_votes_1.head(2))\n",
    "df_votes_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_votes_2 = pd.read_csv(file_votes_2, sep=';')\n",
    "display(df_votes_2.head(2))\n",
    "df_votes_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_following_ignoring = pd.read_csv(file_following_ignoring, sep=';')\n",
    "display(df_following_ignoring.head(2))\n",
    "df_following_ignoring.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation 1: User_A commented/posted to post of User_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_postings = pd.concat([df_postings_1, df_postings_2], ignore_index=True)\n",
    "\n",
    "df_postings[['PostingCreatedAt', 'ArticlePublishingDate', 'UserCreatedAt']] = df_postings[['PostingCreatedAt', 'ArticlePublishingDate', 'UserCreatedAt']].astype('datetime64')\n",
    "\n",
    "df_postings.info()\n",
    "df_postings.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting dataset because it is too large\n",
    "\n",
    "#df_postings['PostingCreatedAt'].dt.date.head()\n",
    "df_postings = df_postings[pd.to_datetime(df_postings['PostingCreatedAt'].dt.date) == '2019-05-01']\n",
    "df_postings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with some NLP extractions of text and one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download spacy german language model - used for tokenization, lemmatization of article text (vector data)\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# encode article channel and article ressort name with one hot encoding\n",
    " \n",
    "# ArticleChannel\tArticleRessortName\n",
    "one_hot_encoder_article_channel = OneHotEncoder(handle_unknown='ignore').fit(df_postings[['ArticleChannel', 'ArticleRessortName']])\n",
    "\n",
    "print(one_hot_encoder_article_channel.categories_)\n",
    "\n",
    "transformed_channel_resort = one_hot_encoder_article_channel.transform(df_postings[['ArticleChannel', 'ArticleRessortName']])\n",
    "print(transformed_channel_resort.shape)\n",
    "display(transformed_channel_resort.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "df_postings['CombinedArticlePostingText'] = df_postings['PostingHeadline'].fillna('') + ' ' + df_postings['PostingComment'].fillna('') + ' ' + df_postings['ArticleTitle'].fillna('')\n",
    "\n",
    "doc_bin = DocBin(attrs=[\"LEMMA\", \"ENT_IOB\", \"ENT_TYPE\"], store_user_data=True)\n",
    "\n",
    "\n",
    "for doc in nlp.pipe(df_postings['CombinedArticlePostingText'].str.lower()):\n",
    "    #print(repr(doc))\n",
    "    doc_bin.add(doc)\n",
    "\n",
    "#bytes_data = doc_bin.to_bytes()\n",
    "#doc_bin = DocBin().from_bytes(bytes_data)\n",
    "\n",
    "file_name_spacy = 'output/combinedPostingText.spacy'\n",
    "doc_bin.to_disk(file_name_spacy)\n",
    "\n",
    "# save tokenized and lemmatized spacy Doc objects to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "doc_bin = DocBin().from_disk(file_name_spacy)\n",
    "\n",
    "docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "#print(docs)\n",
    "print(len(docs))\n",
    "tokenized_lemmatized_texts = [[token.lemma_ for token in doc \n",
    "                               if not token.is_stop and not token.is_punct and not token.is_space and not token.like_url and not token.like_email and not token.like_num] \n",
    "                               for doc in docs]\n",
    "print(len(tokenized_lemmatized_texts))\n",
    "print(tokenized_lemmatized_texts[0])\n",
    "print(tokenized_lemmatized_texts[100])\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), lowercase=False, tokenizer=lambda x: x, max_features=3000)\n",
    "vectorizer = vectorizer.fit(tokenized_lemmatized_texts)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# TODO train test split!\n",
    "# only fit on train data and not on test data!\n",
    "\n",
    "text_vectorized = vectorizer.transform(tokenized_lemmatized_texts)\n",
    "print(text_vectorized.toarray().shape)\n",
    "display(text_vectorized.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
