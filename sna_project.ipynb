{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Network Analysis - Project sna_topic_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option(\"^display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:20,.4f}'.format)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "The data set is provided by Der Standard, one of the top Austrian newspapers.\n",
    "In the online Standard people can post comments below articles and up/down vote comments.\n",
    "\n",
    "1. There are two files containing all **postings** to articles in May 2019 (due to high amount of data, the postings were split into two files). The respective file also contains additionally meta-data of the postings and articles and some details about the users who composed the postings.\n",
    "\n",
    "2. There are two files containing all **votes** for the postings in point 1 (due to high amount of data, the votes were split into two files). The respective file also contains information whether the vote was negative or positive and some details about the user who did the voting.\n",
    "\n",
    "3. There is one file containing **following and ignoring relationships** among all the users who posted (see point 1) or voted (see point 2) to articles published in May 2019. A following relationship (i.e., the user with the `ID_CommunityIdentity` given in column 1 follows the user with the `ID_CommunityIdentityConnectedTo` given in column 2) is indicated by a “1” in column the `“ID_CommunityConnectionType”`, a ignoring relationship by a “2” in that column (i.e., the user with the `ID_CommunityIdentity` given in column 1 ignores the user with the `ID_CommunityIdentityConnectedTo` given in column 2).\n",
    "\n",
    "There are different entities in the data set: \n",
    "* **Users** - identified by *ID_CommunityIdentity* (or *UserCommunityName*)\n",
    "* **Postings** - identified by *ID_Posting*\n",
    "* **Articles** - identified by *ID_Article*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to datasets:\n",
    "file_postings_1 = 'data/Postings_01052019_15052019.csv'\n",
    "file_postings_2 = 'data/Postings_16052019_31052019.csv'\n",
    "file_votes_1 = 'data/Votes_01052019_15052019.csv'\n",
    "file_votes_2 = 'data/Votes_16052019_31052019.csv'\n",
    "file_following_ignoring = 'data/Following_Ignoring_Relationships_01052019_31052019.csv'\n",
    "\n",
    "output_dir = 'output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_postings_1 = pd.read_csv(file_postings_1, sep=';')\n",
    "display(df_postings_1.head(2))\n",
    "df_postings_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_postings_2 = pd.read_csv(file_postings_2, sep=';')\n",
    "display(df_postings_2.head(2))\n",
    "df_postings_2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Concat Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat postings1 and postings2 in order! \n",
    "df_postings = pd.concat([df_postings_1, df_postings_2], ignore_index=True)\n",
    "\n",
    "df_postings[['PostingCreatedAt', 'ArticlePublishingDate', 'UserCreatedAt']] = df_postings[['PostingCreatedAt', 'ArticlePublishingDate', 'UserCreatedAt']].astype('datetime64')\n",
    "\n",
    "df_postings.info()\n",
    "df_postings.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO is it necessary to subset the data because it is too large?\n",
    "\n",
    "#df_postings['PostingCreatedAt'].dt.date.head()\n",
    "# df_postings = df_postings[pd.to_datetime(df_postings['PostingCreatedAt'].dt.date) == '2019-05-01']\n",
    "df_postings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation 1: User_A commented/posted to post of User_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Edge relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of iterating over the dataframe rows to create a edge list, I use a join, which is way faster on the whole dataset. <br>\n",
    "But, by default `merge` changes the sort order, so you have to sort by the original dataframe index!<br>\n",
    "The following code creates a dataframe for posting source user and target posting user. It has the same shape as the original dataframe `df_postings`.\n",
    "So, target posting user may be `NaN`, due to left join and preserving all observations in the posting dataframe.\n",
    "One could filter the Target_User column by not `NaN` to get only relations between users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_user_commented_relation_df = df_postings.reset_index()[['index', 'ID_Posting', 'ID_CommunityIdentity', 'PostingCreatedAt', 'ID_Posting_Parent']].merge(\n",
    "    df_postings[['ID_Posting', 'ID_CommunityIdentity']], \n",
    "    left_on='ID_Posting_Parent', right_on='ID_Posting',\n",
    "    suffixes=('', '_parent'), how='left', sort=False\n",
    ")[['index', 'ID_Posting', 'PostingCreatedAt', 'ID_CommunityIdentity', 'ID_CommunityIdentity_parent']].sort_values(by='index')\n",
    "\n",
    "postings_user_commented_relation_df.set_index('index', inplace=True) # index may be useful for selection?\n",
    "# postings_user_commented_relation_df.set_index(['ID_Posting', 'PostingCreatedAt'], inplace=True) # index may be useful for selection?\n",
    "postings_user_commented_relation_df.rename(columns={'ID_CommunityIdentity': 'Source_User', 'ID_CommunityIdentity_parent': 'Target_User'}, inplace=True)\n",
    "\n",
    "# postings_user_commented_relation_df.drop('index', axis='columns', inplace=True) # index is ascending as original df_postings, index column not needed anymore\n",
    "\n",
    "# SAME number of rows and SAME order as original df_postings:\n",
    "print(df_postings.shape)\n",
    "print(postings_user_commented_relation_df.shape)\n",
    "assert df_postings.shape[0] == postings_user_commented_relation_df.shape[0]\n",
    "\n",
    "# add column PostingCreatedAtDay to have easy access to Day of month\n",
    "postings_user_commented_relation_df['PostingCreatedAtDay'] = postings_user_commented_relation_df['PostingCreatedAt'].dt.day\n",
    "\n",
    "display(postings_user_commented_relation_df.head(5))\n",
    "display(postings_user_commented_relation_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_created_at(series, day_start, day_end):\n",
    "    \"\"\"\n",
    "    create a boolean mask of series where day is in day_start incl. and day_end_incl.\n",
    "    \"\"\"\n",
    "    return series.isin(range(day_start, day_end+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_posting_created_at = get_mask_created_at(postings_user_commented_relation_df['PostingCreatedAtDay'], \n",
    "                                                    day_start=2, day_end=4)\n",
    "\n",
    "display(postings_user_commented_relation_df.loc[mask_posting_created_at].sample(10))\n",
    "list(postings_user_commented_relation_df.loc[mask_posting_created_at]['PostingCreatedAtDay'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful when using the df_posting dataframe without `NAN` values below. Some operations require the original `df_postings` order (NLP text extraction)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "postings_user_commented_relation_nonan_df = postings_user_commented_relation_df.dropna()\n",
    "# check number of rows after dropping NAN\n",
    "print(postings_user_commented_relation_df.shape)\n",
    "print(postings_user_commented_relation_nonan_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute graph topology features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function can compute graph features based on posting relation data frame and start and end day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computation_input_df_combined_postings(postings_relation_df, day_start, day_end):\n",
    "    \"\"\"\n",
    "    day_start incl. and day_end incl.\n",
    "    \"\"\"\n",
    "    print('computation_input_df_combined_postings: Days {}-{}'.format(str(day_start),str(day_end)))\n",
    "    assert (day_start <= day_end) & (day_start > 0) & (day_end <= 31)\n",
    "\n",
    "    postings_relation_df = postings_relation_df.copy()\n",
    "\n",
    "    # TODO remove for speeding up things:\n",
    "    postings_relation_df = postings_relation_df.loc[0:100]\n",
    "\n",
    "    # get postings relations where day is between day_start and day_end\n",
    "    mask_posting_created_at = get_mask_created_at(postings_relation_df['PostingCreatedAtDay'], \n",
    "                                                  day_start=day_start, day_end=day_end)\n",
    "    postings_relation_df_day_subset = postings_relation_df.loc[mask_posting_created_at]\n",
    "    # assert all postings are created between day_start and day_end:\n",
    "    assert postings_relation_df_day_subset['PostingCreatedAtDay'].isin(range(day_start, day_end+1)).all()\n",
    "\n",
    "    # display(postings_relation_df_day_subset.sample(4))\n",
    "\n",
    "    # drop NaN in Target_User, because these rows have no relation:\n",
    "    postings_relation_df_day_subset_no_nan = postings_relation_df_day_subset.dropna().copy()\n",
    "    postings_relation_df_day_subset_no_nan['Target_User'] = postings_relation_df_day_subset_no_nan['Target_User'].astype('int64')\n",
    "\n",
    "    display(postings_relation_df_day_subset_no_nan.sample(4))\n",
    "    display(postings_relation_df_day_subset_no_nan.info())\n",
    "\n",
    "    # obtain interaction count as edge weight\n",
    "    postings_relation_df_day_subset_no_nan_count = postings_relation_df_day_subset_no_nan[['ID_Posting', 'Source_User', 'Target_User']].groupby(\n",
    "                                                        ['Source_User', 'Target_User'])['ID_Posting'].count().reset_index(name=\"weight\")\n",
    "\n",
    "    display(postings_relation_df_day_subset_no_nan_count.sample(4))\n",
    "    #display(postings_relation_df_day_subset_no_nan_count.info())\n",
    "\n",
    "    # Creation of directed graph for day range\n",
    "    G_postings_commented_days_sub_directed = nx.from_pandas_edgelist(postings_relation_df_day_subset_no_nan_count,\n",
    "                                                                     source='Source_User',\n",
    "                                                                     target='Target_User',\n",
    "                                                                     edge_attr='weight',\n",
    "                                                                     create_using=nx.DiGraph())\n",
    "    print('Created a', nx.info(G_postings_commented_days_sub_directed))\n",
    "    # assert that the graph has weight as edge attribute with a weight > 0 (should be weight column in df)\n",
    "    assert G_postings_commented_days_sub_directed.get_edge_data(*tuple(postings_relation_df_day_subset_no_nan_count.loc[0, ['Source_User', 'Target_User']]))['weight'] > 0\n",
    "\n",
    "    G_postings_commented_days_sub_undirected = nx.from_pandas_edgelist(postings_relation_df_day_subset_no_nan_count,\n",
    "                                                                       source='Source_User',\n",
    "                                                                       target='Target_User',\n",
    "                                                                       create_using=nx.Graph())\n",
    "    print('Created a', nx.info(G_postings_commented_days_sub_undirected))\n",
    "\n",
    "    # Computation of Jaccard coefficient\n",
    "    jac = nx.jaccard_coefficient(G_postings_commented_days_sub_undirected)\n",
    "    jac_df = pd.DataFrame(list(jac), columns = ['Source_User', 'Target_User', 'jaccard_coef'])\n",
    "    print('Jaccard: ok!')\n",
    "\n",
    "    # Computation of Adamic and Adar index\n",
    "    ad_ad = nx.adamic_adar_index(G_postings_commented_days_sub_undirected)\n",
    "    ad_ad_df = pd.DataFrame(list(ad_ad), columns = ['Source_User', 'Target_User', 'adamic_adar_index'])\n",
    "    print('Adamic and Adar: ok!')\n",
    "    result_df = jac_df.merge(ad_ad_df)\n",
    "\n",
    "    # Computation of Preferential Attachment\n",
    "    pref_att = nx.preferential_attachment(G_postings_commented_days_sub_undirected)\n",
    "    pref_att_df = pd.DataFrame(list(pref_att), columns = ['Source_User', 'Target_User', 'preferential_attachment_index'])\n",
    "    print('Preferential Attachment: ok!')\n",
    "    result_df = result_df.merge(pref_att_df)\n",
    "\n",
    "    # Clustering coefficient score (on directed graph)\n",
    "    cluster_dict = nx.clustering(G_postings_commented_days_sub_directed)\n",
    "    cluster_dict_df = pd.DataFrame.from_dict(cluster_dict, orient='index', columns=['clustering_coefficient_score']).reset_index().rename(columns={'index':'User'})\n",
    "    #display(cluster_dict_df.head())\n",
    "    # Add a column for clustering_coefficient_score_Source_User using the Clustering coefficient for the Source_User node\n",
    "    result_df = result_df.merge(cluster_dict_df.rename(columns={'User': 'Source_User', \n",
    "                                                               'clustering_coefficient_score': 'clustering_coefficient_score_Source_User'}),\n",
    "                                on='Source_User', how='left')\n",
    "    # Add a column for clustering_coefficient_score_Target_User using the Clustering coefficient for the Target_User node\n",
    "    result_df = result_df.merge(cluster_dict_df.rename(columns={'User': 'Target_User', \n",
    "                                                               'clustering_coefficient_score': 'clustering_coefficient_score_Target_User'}),\n",
    "                                on='Target_User', how='left')\n",
    "    print('Clustering coefficient: ok!')\n",
    "    \n",
    "    # Pagerank (on directed graph)\n",
    "    pagerank_dict = nx.pagerank(G_postings_commented_days_sub_directed)\n",
    "    pagerank_dict_df = pd.DataFrame.from_dict(pagerank_dict, orient='index', columns=['pagerank']).reset_index().rename(columns={'index':'User'})\n",
    "     # Add a column for pagerank_Source_User using the Pagerank for the Source_User node\n",
    "    result_df = result_df.merge(pagerank_dict_df.rename(columns={'User': 'Source_User', \n",
    "                                                               'pagerank': 'pagerank_Source_User'}),\n",
    "                                on='Source_User', how='left')\n",
    "    # Add a column for pagerank_Target_User using the Pagerank for the Target_User node\n",
    "    result_df = result_df.merge(pagerank_dict_df.rename(columns={'User': 'Target_User', \n",
    "                                                               'pagerank': 'pagerank_Target_User'}),\n",
    "                                on='Target_User', how='left')\n",
    "    print('Pagerank: ok!')\n",
    "    print('Resulting result_df has shape', result_df.shape)\n",
    "    display(result_df.head(5))\n",
    "    #display(result_df[result_df['clustering_coefficient_score_Target_User'] > 0])\n",
    "\n",
    "    # Write resulting graph topology features for Source_User and Target_User combinations to a file\n",
    "    import pickle\n",
    "    pickle_out_file_path_features = 'output/G_features_postings_days_{}-{}.pkl'.format(str(day_start), str(day_end))\n",
    "    with open(pickle_out_file_path_features, 'wb') as f:\n",
    "        pickle.dump(result_df, f)\n",
    "    print(f'Successfully written result_df to file \"{pickle_out_file_path_features}\".')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate for days and save to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_graph_features = computation_input_df_combined_postings(postings_user_commented_relation_df, day_start=1, day_end=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with some NLP extractions of text and one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download spacy german language model - used for tokenization, lemmatization of article text (vector data)\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot encoding (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot encoding of `'ArticleChannel'`, `'ArticleRessortName'` for ML pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# encode article channel and article ressort name with one hot encoding\n",
    "\n",
    "# TODO use it like that or use the TEST set for one hot encoding. \n",
    "# Assuming all article channels and article resort names are known a priori this is ok imho for derStandard.\n",
    "# Just use a subset of df_posting e.g. certain days, if the number of features is too high in ML pipeline...\n",
    "\n",
    "# ArticleChannel\tArticleRessortName\n",
    "one_hot_encoder_article_channel = OneHotEncoder(handle_unknown='ignore').fit(df_postings[['ArticleChannel', 'ArticleRessortName']])\n",
    "\n",
    "print(one_hot_encoder_article_channel.categories_)\n",
    "\n",
    "transformed_channel_resort = one_hot_encoder_article_channel.transform(df_postings[['ArticleChannel', 'ArticleRessortName']])\n",
    "print(transformed_channel_resort.shape)\n",
    "display(transformed_channel_resort.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF feature extraction (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF feature extraction of combined `PostingHeadline`, `PostingComment` and `ArticleTitle` next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_spacy = 'output/combinedPostingText.spacy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step for creating `'output/combinedPostingText.spacy'` is usually not necessary. It takes around 64 minutes and you can just use my file: <br>\n",
    "Run the next chunk, if you want to wait long for computing the `\"combinedPostingText.spacy\"` file.<br>\n",
    "`doc_bin` holds document information in order of `df_postings`. Existing `combinedPostingText.spacy` stores nlp information for all `df_postings` rows, not just a subset of days. Take care of the order of `df_postings`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "df_postings['CombinedArticlePostingText'] = df_postings['PostingHeadline'].fillna('') + ' ' + df_postings['PostingComment'].fillna('') + ' ' + df_postings['ArticleTitle'].fillna('')\n",
    "\n",
    "doc_bin = DocBin(attrs=[\"LEMMA\", \"ENT_IOB\", \"ENT_TYPE\"], store_user_data=True)\n",
    "\n",
    "for doc in nlp.pipe(df_postings['CombinedArticlePostingText'].str.lower()):\n",
    "    #print(repr(doc))\n",
    "    doc_bin.add(doc)\n",
    "\n",
    "#bytes_data = doc_bin.to_bytes()\n",
    "#doc_bin = DocBin().from_bytes(bytes_data)\n",
    "\n",
    "doc_bin.to_disk(file_name_spacy)\n",
    "\n",
    "# save tokenized spacy Doc objects to file\n",
    "# the file contains the combined text of posting headline, posting comment and article title in a tokenized form\n",
    "# and also meta data about a token, like its lemma form, whether a token is a punctuation, or bracket etc.\n",
    "# processing took around 64 minutes on the whole df_postings for me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next takes around 5 minutes for me. You could also just use the pickle file `tokenized_lemmatized_texts.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "doc_bin = DocBin().from_disk(file_name_spacy)\n",
    "\n",
    "docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "#print(docs)\n",
    "print(len(docs))\n",
    "tokenized_lemmatized_texts = [[token.lemma_ for token in doc \n",
    "                               if not token.is_stop and not token.is_punct and not token.is_space \n",
    "                               and not token.like_url and not token.like_email and not token.is_quote \n",
    "                               and not token.is_currency and not token.is_bracket and not token.is_quote\n",
    "                               and not token.is_left_punct and not token.is_right_punct and not token.is_bracket]\n",
    "                               for doc in docs]\n",
    "print(len(tokenized_lemmatized_texts))\n",
    "print(tokenized_lemmatized_texts[0])\n",
    "print(tokenized_lemmatized_texts[100])\n",
    "\n",
    "# tokenized_lemmatized_texts contains nlp information for all documents in df_postings in order!\n",
    "\n",
    "# save as pickle file:\n",
    "import pickle\n",
    "with open('output/tokenized_lemmatized_texts.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenized_lemmatized_texts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue here: Load the pickled file `tokenized_lemmatized_texts.pkl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('output/tokenized_lemmatized_texts.pkl', 'rb') as f:\n",
    "    tokenized_lemmatized_texts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_lemmatized_texts contains nlp information for all documents in df_postings in order!\n",
    "# is a list of list of tokens, a list of tokens for the combined texts for a posting-article-combination for a certain posting entry in posting_df.\n",
    "print(tokenized_lemmatized_texts[0:2])\n",
    "print(len(tokenized_lemmatized_texts))\n",
    "print(len(df_postings))\n",
    "\n",
    "assert len(tokenized_lemmatized_texts) == len(df_postings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter tokenized_lemmatized_texts by days of PostingCreatedAt:\n",
    "# TODO adjust to TRAIN or TEST set in use for feature extraction:\n",
    "from itertools import compress\n",
    "\n",
    "mask_posting_created_at_days_1_to_5 = get_mask_created_at(postings_user_commented_relation_df['PostingCreatedAtDay'],\n",
    "                                                          day_start=1, day_end=5)\n",
    "tokenized_lemmatized_texts_at_days_1_to_5 = list(compress(tokenized_lemmatized_texts, list(mask_posting_created_at_days_1_to_5)))\n",
    "\n",
    "assert (len(tokenized_lemmatized_texts_at_days_1_to_5)) == mask_posting_created_at_days_1_to_5.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO train test split!\n",
    "# only fit on train data and not on test data!\n",
    "# \n",
    "max_tf_idf_features = 300\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), lowercase=False, tokenizer=lambda x: x, max_features=max_tf_idf_features)\n",
    "vectorizer = vectorizer.fit(tokenized_lemmatized_texts)\n",
    "#print(list(vectorizer.get_feature_names_out()))\n",
    "\n",
    "# text_vectorized = vectorizer.transform(tokenized_lemmatized_texts)\n",
    "#print(text_vectorized.toarray().shape)\n",
    "#display(text_vectorized.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a check to see which tf-idf features were extracted:\n",
    "with open('output/feature_names.txt', 'w') as f:\n",
    "    for item in vectorizer.get_feature_names_out():\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
